{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Dropout\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas.core.indexing import _IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.core.dtypes.common import is_numeric_dtype\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "idx: _IndexSlice = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the data\n",
    "DATA_PATH = Path(\"data/01_known_effects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of samples\n",
    "samples = pd.read_csv(\n",
    "    DATA_PATH / \"experiments.csv\",\n",
    "    index_col=0,\n",
    "    decimal=\",\"\n",
    "    )\n",
    "\n",
    "# Create an index from the experiment table\n",
    "samples_index_fields = {\n",
    "        'Label': \"label\",\n",
    "        'Effect in PSET':\"effect\",\n",
    "        'Treatment': \"treatment\",\n",
    "        'Experimenter, location': \"experimenter_location\",\n",
    "        'Strain': \"strain\",\n",
    "        'CO2 level': \"CO2_level\",\n",
    "        'Cultivation + experiment temperature': \"temperature\",\n",
    "        'Cultivation light intensity': \"Light_intensity\",\n",
    "        'Dark or light acclimated': \"light_acclimation\",\n",
    "        'Growth light color (nm)': \"light_color\",\n",
    "        'Cultivator': \"cultivator\",\n",
    "        'Medium': \"medium\",\n",
    "        'Fluorometer': \"fluorometer\",\n",
    "        'SP color (nm)': \"SP_color\",\n",
    "        'SP intensity': \"SP_intensity\",\n",
    "        'Measuring vessel': \"vessel\",\n",
    "        'OD680 MC-1000': \"OD680\",\n",
    "        'OD720 MC-1000': \"OD720\",\n",
    "        'Î”OD': \"deltaOD\",\n",
    "        'OD680/720 raw': \"OD680_720\",\n",
    "}\n",
    "samples_index_fields_inv = {v:k for k,v in samples_index_fields.items()}\n",
    "\n",
    "samples_index = pd.MultiIndex.from_frame(\n",
    "    samples.reset_index().loc[:, list(samples_index_fields.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_levels(df, level):\n",
    "    columns = df.columns.remove_unused_levels()\n",
    "    \n",
    "    # Get the levels and the index corresponding to the level\n",
    "    _level = samples_index_fields_inv[level]\n",
    "    level_ind = np.where(np.array(list(df.columns.names)) == _level)[0][0]\n",
    "    return list(columns.levels[level_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(samples.columns[2:])/3)), \n",
    "    3,\n",
    "    figsize = (7, 15)\n",
    ")\n",
    "\n",
    "for column, ax in zip(samples.columns[2:], axes.flatten()):\n",
    "    # Get the values dor each column and count the occurrences\n",
    "    dat = samples.loc[:,column]\n",
    "    if not is_numeric_dtype(dat.dtype):\n",
    "        dat = dat.value_counts()\n",
    "        dat.plot(kind=\"bar\", ax=ax)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=10, ha='right')\n",
    "    else:\n",
    "        dat.plot(kind=\"hist\", ax=ax)\n",
    "\n",
    "    ax.set_title(column, size=10)\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    wspace = 0.3,\n",
    "    hspace=0.7\n",
    ")\n",
    "\n",
    "for column in samples.columns[:2]:\n",
    "    fig, ax = plt.subplots()\n",
    "    dat = samples.loc[:,column]\n",
    "    dat = dat.value_counts()\n",
    "    dat.plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.set_title(column, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in samples_index_fields.values():\n",
    "#     print(f\"{i},\")# : str | list[str | int] | slice | None = slice(None),\")\n",
    "\n",
    "# Create a function for easy indexing\n",
    "def didx(\n",
    "    label: str | list[str | int] | slice | None = slice(None),\n",
    "    effect: str | list[str | int] | slice | None = slice(None),\n",
    "    treatment: str | list[str | int] | slice | None = slice(None),\n",
    "    experimenter_location: str | list[str | int] | slice | None = slice(None),\n",
    "    strain: str | list[str | int] | slice | None = slice(None),\n",
    "    CO2_level: str | list[str | int] | slice | None = slice(None),\n",
    "    temperature: str | list[str | int] | slice | None = slice(None),\n",
    "    Light_intensity: str | list[str | int] | slice | None = slice(None),\n",
    "    light_acclimation: str | list[str | int] | slice | None = slice(None),\n",
    "    light_color: str | list[str | int] | slice | None = slice(None),\n",
    "    cultivator: str | list[str | int] | slice | None = slice(None),\n",
    "    medium: str | list[str | int] | slice | None = slice(None),\n",
    "    fluorometer: str | list[str | int] | slice | None = slice(None),\n",
    "    SP_color: str | list[str | int] | slice | None = slice(None),\n",
    "    SP_intensity: str | list[str | int] | slice | None = slice(None),\n",
    "    vessel: str | list[str | int] | slice | None = slice(None),\n",
    "    OD680: str | list[str | int] | slice | None = slice(None),\n",
    "    OD720: str | list[str | int] | slice | None = slice(None),\n",
    "    deltaOD: str | list[str | int] | slice | None = slice(None),\n",
    "    OD680_720: str | list[str | int] | slice | None = slice(None),\n",
    ") -> int | str | slice:\n",
    "    res: list[str | list[str | int] | slice | None] = [\n",
    "        x\n",
    "        for x in [\n",
    "            label,\n",
    "            effect,\n",
    "            treatment,\n",
    "            experimenter_location,\n",
    "            strain,\n",
    "            CO2_level,\n",
    "            temperature,\n",
    "            Light_intensity,\n",
    "            light_acclimation,\n",
    "            light_color,\n",
    "            cultivator,\n",
    "            medium,\n",
    "            fluorometer,\n",
    "            SP_color,\n",
    "            SP_intensity,\n",
    "            vessel,\n",
    "            OD680,\n",
    "            OD720,\n",
    "            deltaOD,\n",
    "            OD680_720,\n",
    "        ]\n",
    "        if x is not None\n",
    "    ]\n",
    "    return idx[*res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths to the samples files\n",
    "files = {}\n",
    "\n",
    "for i in samples_index.get_level_values(0):\n",
    "    _index = f\"{i:04}\"\n",
    "\n",
    "    # Get the file to the current index\n",
    "    file = list(DATA_PATH.glob(f\"ojip_data/{_index}*\"))[0]\n",
    "\n",
    "    # Set the options for reading the data based on the used fluorometer\n",
    "    if samples.loc[i, \"Fluorometer\"] == \"MULTI-COLOR-PAM\":\n",
    "        skiprows = 0\n",
    "        skipfooter = 0\n",
    "        sep=\";\"\n",
    "        index_col=0\n",
    "        select_col = \"Fluo, V\"\n",
    "        time_to_ms = 1\n",
    "    elif samples.loc[i, \"Fluorometer\"] == \"AquaPen\":\n",
    "        skiprows = 7\n",
    "        skipfooter = 38\n",
    "        sep=r\"\\s\"\n",
    "        index_col=0\n",
    "        select_col = \"OJIP\"\n",
    "        time_to_ms = 1e-3\n",
    "    else:\n",
    "        print(i, samples.loc[i, \"Fluorometer\"])\n",
    "        break\n",
    "\n",
    "    # Read the data with the pre-defined options\n",
    "    _df = pd.read_table(\n",
    "        file,\n",
    "        skiprows=skiprows,\n",
    "        skipfooter=skipfooter,\n",
    "        sep=sep,\n",
    "        index_col=index_col,\n",
    "        engine='c' if skipfooter == 0 else 'python'\n",
    "    )[select_col]\n",
    "\n",
    "    _df.index = pd.Index(np.round(_df.index * time_to_ms, 2))\n",
    "\n",
    "    # Save the data\n",
    "    files[i] = _df\n",
    "\n",
    "# Concatenate the data\n",
    "df = pd.DataFrame(files).sort_index(axis=1)\n",
    "df.columns = samples_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scaling factors from the fluorometer comparison data\n",
    "SCALING_DATA_PATH = Path(\"data/00_fluorometer_data_scaling\")\n",
    "\n",
    "scaling_samples = pd.read_csv(\n",
    "    SCALING_DATA_PATH / \"experiments.csv\",\n",
    "    index_col=0,\n",
    "    decimal=\",\"\n",
    "    )\n",
    "\n",
    "# Get the paths to the samples files\n",
    "files = {}\n",
    "\n",
    "for i in scaling_samples.index:\n",
    "    _index = f\"{i:02}\"\n",
    "\n",
    "    # Get the file to the current index\n",
    "    file = list(SCALING_DATA_PATH.glob(f\"ojip_data/{_index}*\"))[0]\n",
    "\n",
    "    # Set the options for reading the data based on the used fluorometer\n",
    "    if scaling_samples.loc[i, \"Fluorometer\"] == \"MULTI-COLOR-PAM\":\n",
    "        skiprows = 0\n",
    "        skipfooter = 0\n",
    "        sep=\";\"\n",
    "        index_col=0\n",
    "        select_col = \"Fluo, V\"\n",
    "        time_to_ms = 1\n",
    "    elif scaling_samples.loc[i, \"Fluorometer\"] == \"AquaPen\":\n",
    "        skiprows = 7\n",
    "        skipfooter = 38\n",
    "        sep=r\"\\s\"\n",
    "        index_col=0\n",
    "        select_col = \"OJIP\"\n",
    "        time_to_ms = 1e-3\n",
    "    else:\n",
    "        print(i, scaling_samples.loc[i, \"Fluorometer\"])\n",
    "        break\n",
    "\n",
    "    # Read the data with the pre-defined options\n",
    "    _df = pd.read_table(\n",
    "        file,\n",
    "        skiprows=skiprows,\n",
    "        skipfooter=skipfooter,\n",
    "        sep=sep,\n",
    "        index_col=index_col,\n",
    "        engine='c' if skipfooter == 0 else 'python'\n",
    "    )[select_col]\n",
    "\n",
    "    _df.index = pd.Index(np.round(_df.index * time_to_ms, 2))\n",
    "\n",
    "    # Save the data\n",
    "    files[i] = _df\n",
    "\n",
    "# Concatenate the data\n",
    "scaling_df = pd.DataFrame(files).sort_index(axis=1)\n",
    "scaling_df.columns = pd.MultiIndex.from_frame(scaling_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine scaling factors between devices \n",
    "factors = pd.DataFrame(index=pd.MultiIndex([[],[],[],[]], [[],[],[],[]], name=[\"from\", \"from SP (nm)\", \"to\", \"to SP (nm)\"]))\n",
    "\n",
    "for hue in scaling_df.columns.levels[-1]:\n",
    "    _dat = scaling_df.loc[:,idx[:,:,:,hue]].loc[0:0.05].mean().droplevel([0,-1])\n",
    "    MCPAM = _dat.loc[idx[[\"MULTI-COLOR-PAM\"], :]]\n",
    "    AQPEN = _dat.loc[idx[[\"AquaPen\"], :]]\n",
    "    \n",
    "    factors.loc[idx[*AQPEN.index[0], *MCPAM.index[0]], \"F0\"] = (\n",
    "        MCPAM.iloc[0] / AQPEN.iloc[0]\n",
    "    )\n",
    "\n",
    "# factors[\"Fm\"] = (\n",
    "#     scaling_df.loc[:, idx[:, \"MULTI-COLOR-PAM\",:]].max().droplevel([0,1,2])\n",
    "#     / scaling_df.loc[:, idx[:, \"AquaPen\",:]].max().droplevel([0,1,2])\n",
    "# )\n",
    "\n",
    "# factors = pd.DataFrame(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine scaling factors between devices at F0 and FM\n",
    "factors = pd.DataFrame(index=pd.MultiIndex([[],[],[],[]], [[],[],[],[]], name=[\"from\", \"from SP (nm)\", \"to\", \"to SP (nm)\"]))\n",
    "\n",
    "for hue in scaling_df.columns.levels[-1]:\n",
    "    MCPAM = scaling_df.loc[:,idx[:,\"MULTI-COLOR-PAM\",:,hue]].droplevel([0,-1], axis=1)\n",
    "    AQPEN = scaling_df.loc[:,idx[:,\"AquaPen\",:,hue]].droplevel([0,-1], axis=1)\n",
    "    \n",
    "    # F0\n",
    "    factors.loc[idx[*AQPEN.columns[0], *MCPAM.columns[0]], \"F0\"] = (\n",
    "        MCPAM.loc[0:0.05].mean().iloc[0] / AQPEN.loc[0:0.05].mean().iloc[0]\n",
    "    )\n",
    "\n",
    "    # FM\n",
    "    factors.loc[idx[*AQPEN.columns[0], *MCPAM.columns[0]], \"FM\"] = (\n",
    "        MCPAM.max().iloc[0] / AQPEN.max().iloc[0]\n",
    "    )\n",
    "\n",
    "factors[\"mean\"] = factors.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale AquaPen samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale AquaPen samples with the determined conversion factors\n",
    "for (fluorometer, SP_color, _, _), factor in factors[\"mean\"].items():\n",
    "    try:\n",
    "        df.loc[:, didx(fluorometer=fluorometer, SP_color=SP_color)] = df.loc[:, didx(fluorometer=fluorometer, SP_color=SP_color)] * factor\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plots\n",
    "\n",
    "ax = df.loc[:, didx(\n",
    "    fluorometer=\"MULTI-COLOR-PAM\",\n",
    "    CO2_level=\"Air\", \n",
    "    strain=\"Chlorella vulgaris\",\n",
    "    # SP_color=455\n",
    ")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [Detector V]\")\n",
    "ax.set_title(\"MCPAM - Example\")\n",
    "\n",
    "ax = df.loc[:, didx(fluorometer=\"AquaPen\", CO2_level=\"Air\")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [AU]\")\n",
    "ax.set_title(\"AquaPen - Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the data to be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.loc[\n",
    "    0.01:, # Exclude data before the light pulse\n",
    "    didx(\n",
    "        fluorometer=\"MULTI-COLOR-PAM\", # Only use MCPAM data\n",
    "        strain='Synechocystis sp. PCC 6803', # Only use Synechocystis data\n",
    "    )\n",
    "].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metrics of the different models\n",
    "models_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the map of effects transformed into one-hot encoding\n",
    "effects_map = pd.read_csv(\n",
    "    DATA_PATH / \"effects_map.csv\",\n",
    "    header=[0,1],\n",
    "    index_col=[0,1],\n",
    "    ).astype(float).fillna(0).astype(bool)\n",
    "\n",
    "# Get the effects and map the mto the targets\n",
    "effects = samples.loc[dat.columns.get_level_values(0), [\"Effect in PSET\", \"Treatment\"]]\n",
    "\n",
    "df_targets = effects_map.loc[pd.MultiIndex.from_frame(effects)]\n",
    "df_targets.index = dat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for model evaluation\n",
    "\n",
    "# Plot the loss development over epochs\n",
    "def plot_loss_development(history):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the loss curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "    plt.plot(val_loss, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Function Development Over Epochs')\n",
    "    plt.ylim(0)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Get the true/false positives/negatives if the data is one-hot encoded\n",
    "def get_onehot_prediction_error(Y_pred, Y_test, prediction_threshold):\n",
    "    # Set the values over the prediction_threshold as True\n",
    "    Y_pred_bool = (Y_pred > prediction_threshold).astype(int)\n",
    "\n",
    "    # Calculate the hits and errors\n",
    "    Y_pred_err = (Y_test - Y_pred_bool * 2)\n",
    "    Y_pred_err = Y_pred_err.apply(lambda x: x.value_counts()).astype(float)\n",
    "\n",
    "    # Map the comparison values to their meaning\n",
    "    error_map = {\n",
    "        -2: \"false positive\",\n",
    "        -1: \"true positive\",\n",
    "        0: \"true negative\",\n",
    "        1: \"false negative\"\n",
    "    }\n",
    "    Y_pred_err.index = pd.Index([error_map[x] for x in Y_pred_err.index ])\n",
    "    return Y_pred_err\n",
    "\n",
    "# Get the metrics Accuracy, Precision, Recall, and F1-Score\n",
    "def get_model_metrics(Y_test, Y_pred, prediction_threshold):\n",
    "    # Set the values over the prediction_threshold as True\n",
    "    Y_pred_bool = (Y_pred > prediction_threshold).astype(int)\n",
    "\n",
    "    model_metrics = pd.DataFrame(\n",
    "        columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n",
    "        index = pd.MultiIndex([[],[]], [[],[]])\n",
    "    )\n",
    "\n",
    "    for column in list(Y_test.columns.get_level_values(0)) + [\"total\"]:\n",
    "        if column == \"total\":\n",
    "            Y_test_long = Y_test.droplevel(1, axis=1).stack()\n",
    "            Y_pred_bool_long = Y_pred_bool.droplevel(1, axis=1).stack()\n",
    "        else:\n",
    "            Y_test_long = Y_test.droplevel(1, axis=1)[column]\n",
    "            Y_pred_bool_long = Y_pred_bool.droplevel(1, axis=1)[column]\n",
    "\n",
    "\n",
    "        # Total scores\n",
    "        accuracy = accuracy_score(Y_test_long, Y_pred_bool_long )\n",
    "\n",
    "        # Calculate precision\n",
    "        if Y_pred_bool_long.sum() == 0:\n",
    "            precision = np.nan\n",
    "        else:\n",
    "            precision = precision_score(Y_test_long, Y_pred_bool_long )\n",
    "\n",
    "        # Calculate recall (sensitivity)\n",
    "        recall = recall_score(Y_test_long, Y_pred_bool_long )\n",
    "\n",
    "        # Calculate F1-score\n",
    "        f1 = f1_score(Y_test_long, Y_pred_bool_long )\n",
    "\n",
    "        #idx[column, f\"n={int((Y_pred_bool_long!= 0).sum())}\"]\n",
    "\n",
    "        model_metrics.loc[idx[column, f\"n={int((Y_test_long!= 0).sum())}\"], :] = (accuracy, precision, recall, f1)\n",
    "\n",
    "    # # Add the number of non-zero samples\n",
    "    # model_metrics.loc[:, \"n\"] = (Y_test != 0).sum().droplevel(1)\n",
    "\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network - Training with Feature selection\n",
    "Exclude AquaPen data until a conversion is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tran2024, nine features were extracted:\n",
    "- F0\n",
    "- Fm\n",
    "- F(50 Âµs)\n",
    "- F(100 Âµs)\n",
    "- F(300 Âµs)\n",
    "- F(2 ms)\n",
    "- F(30 ms)\n",
    "- Timing of Fm\n",
    "- Area above the curve between F0 and Fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential other features:\n",
    "- Log-spaced subsampling\n",
    "- Timing of inflection points (I, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature table\n",
    "df_features = pd.DataFrame(index=dat.columns, columns=pd.MultiIndex([[],[]], [[],[]], dtype=[str, float]))\n",
    "\n",
    "# F0 [AU]\n",
    "df_features.loc[:, idx[\"F0 [AU]\", np.nan]] = dat.iloc[:3].mean()\n",
    "\n",
    "# Fm [AU]\n",
    "df_features.loc[:, idx[\"Fm [AU]\", np.nan]] = dat.max()\n",
    "\n",
    "# Fm timing [ms]\n",
    "df_features.loc[:, idx[\"Fm time [ms]\", np.nan]] = dat.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of sampled points\n",
    "n_points = 40\n",
    "\n",
    "# Time points, logspaced\n",
    "time_points = np.logspace(\n",
    "    np.log10(dat.index[0]),\n",
    "    np.log10(dat.index[-1]),\n",
    "    n_points\n",
    ")[1:]\n",
    "\n",
    "# Pre-populate the interp function\n",
    "_interp = partial(np.interp, time_points, dat.index)\n",
    "\n",
    "# Interpolate the selected points\n",
    "F_sampled = dat.apply(_interp)\n",
    "F_sampled.index = pd.MultiIndex.from_product([[\"Fsampled\"], time_points])\n",
    "\n",
    "# Add sampled points to features\n",
    "df_features = pd.concat([df_features, F_sampled.T], axis=1)\n",
    "\n",
    "# Plot the sampling points\n",
    "# Subset the data to the samples and time to be included in the analysis \n",
    "ax = dat.plot(legend=False)\n",
    "\n",
    "for t in time_points:\n",
    "    ax.axvline(t)\n",
    "\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for UMAP\n",
    "UMAP_seed = 2025\n",
    "\n",
    "# Scale the features\n",
    "df_features_scaled = StandardScaler().fit_transform(df_features.values)\n",
    "\n",
    "# Create the UMAP embedding\n",
    "reducer = umap.UMAP(random_state=UMAP_seed)\n",
    "embedding = pd.DataFrame(\n",
    "    reducer.fit_transform(df_features_scaled),\n",
    "    index=df_features.index,\n",
    "    columns=[\"UMAP_1\", \"UMAP_2\"]\n",
    ").reset_index()\n",
    "\n",
    "# Plot\n",
    "categories = df.columns.names[1:]\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,15),\n",
    "    sharey=True,\n",
    "    sharex=True,\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.scatterplot(\n",
    "    embedding,\n",
    "    x=\"UMAP_1\",\n",
    "    y=\"UMAP_2\",\n",
    "    hue=\"Effect in PSET\",\n",
    "    ax=ax,\n",
    ")\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UMAP to targets\n",
    "embedding_targets = pd.concat([\n",
    "    df_targets.droplevel(-1, axis=1).droplevel(list(range(1,20)), axis=0),\n",
    "    embedding.set_index(\"Label\").loc[:, [\"UMAP_1\", \"UMAP_2\"]],\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "categories = effects_map.columns.get_level_values(0)\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,7),\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding_targets,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding_targets[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df_features.astype(int),\n",
    "    df_targets.astype(int), \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_targets.values\n",
    ")\n",
    "\n",
    "if not np.all(Y_train.drop_duplicates().sum(axis=0) == 1):\n",
    "    raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# Scale data\n",
    "scaler_X = StandardScaler().fit(X_train.values)\n",
    "scaler_Y = StandardScaler().fit(Y_train.values)\n",
    "\n",
    "X_train_scaled = scaler_X.transform(X_train.values)\n",
    "X_test_scaled = scaler_X.transform(X_test.values)\n",
    "\n",
    "Y_train_scaled = scaler_Y.transform(Y_train.values)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test.values)\n",
    "\n",
    "print(f\"Training model to recognize {Y_train.shape[1]} target features.\\nUsing {X_train.shape[0]} samples with {X_train.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input((X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),  # Input layer\n",
    "    Dense(64, activation='relu'),  # Hidden layer\n",
    "    Dense(Y_train_scaled.shape[1])  # Output layer with no activation (for regression)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # MSE for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    Y_train_scaled, \n",
    "    validation_data=(X_test_scaled, Y_test_scaled),\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    callbacks=[TqdmCallback(verbose=1)]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, mae = model.evaluate(X_test_scaled, Y_test_scaled, verbose=0)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")\n",
    "\n",
    "# Plot the loss over the Epochs\n",
    "plot_loss_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "Y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Reverse scale\n",
    "Y_pred = pd.DataFrame(\n",
    "    scaler_Y.inverse_transform(Y_pred_scaled),\n",
    "    index=Y_test.index,\n",
    "    columns=Y_test.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold over which the prediction is assessed as True\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "# Get the prediction errors for the one-hot encoded targets\n",
    "Y_pred_err = get_onehot_prediction_error(Y_pred, Y_test, prediction_threshold)\n",
    "\n",
    "# Plot the errors\n",
    "ax = Y_pred_err.droplevel(1, axis=1).T.plot(kind=\"bar\")\n",
    "ax.set_title(f\"Validation prediction with threshold {prediction_threshold}\")\n",
    "\n",
    "# Calculate the model metrics\n",
    "model_metrics = get_model_metrics(Y_test, Y_pred, prediction_threshold)\n",
    "models_metrics[\"FNN\"] = model_metrics\n",
    "\n",
    "# Plot the model metrics\n",
    "model_metrics.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model - OJIP time-course learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of sampled points\n",
    "n_points = 40\n",
    "\n",
    "# Time points, logspaced\n",
    "time_points = np.logspace(\n",
    "    np.log10(dat.index[0]),\n",
    "    np.log10(dat.index[-1]),\n",
    "    n_points\n",
    ")[1:]\n",
    "\n",
    "# Pre-populate the interp function\n",
    "_interp = partial(np.interp, time_points, dat.index)\n",
    "\n",
    "# Interpolate the selected points\n",
    "F_sampled = dat.apply(_interp)\n",
    "F_sampled.index = pd.MultiIndex.from_product([[\"Fsampled\"], time_points])\n",
    "\n",
    "# Add sampled points to features\n",
    "df_features = F_sampled.T\n",
    "\n",
    "# Subset the data to the samples and time to be included in the analysis \n",
    "ax = dat.plot(legend=False)\n",
    "\n",
    "for t in time_points:\n",
    "    ax.axvline(t)\n",
    "\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df_features.astype(int),\n",
    "    df_targets.astype(int), \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_targets.values\n",
    ")\n",
    "\n",
    "if not np.all(Y_train.drop_duplicates().sum(axis=0) == 1):\n",
    "    raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# Scale data\n",
    "scaler_X = StandardScaler().fit(X_train.values)\n",
    "scaler_Y = StandardScaler().fit(Y_train.values)\n",
    "\n",
    "X_train_scaled = scaler_X.transform(X_train.values)\n",
    "X_test_scaled = scaler_X.transform(X_test.values)\n",
    "\n",
    "Y_train_scaled = scaler_Y.transform(Y_train.values)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test.values)\n",
    "\n",
    "print(f\"Training model to recognize {Y_train.shape[1]} target features.\\nUsing {X_train.shape[0]} samples with {X_train.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing our recurrent neural network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer\n",
    "\n",
    "model.add(Input(shape=(X_train_scaled.shape[1], 1)))\n",
    "\n",
    "# #Adding LSTM layers\n",
    "model.add(LSTM(units = 32, activation='tanh', return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 32, activation='tanh', return_sequences = False))\n",
    "\n",
    "# Perform some dropout regularization\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Adding our output layer\n",
    "\n",
    "# model.add(Dense(16, activation='relu'))  # Fully connected layer\n",
    "model.add(Dense(units = Y_train_scaled.shape[1], activation='linear'))\n",
    "\n",
    "#Compiling the recurrent neural network\n",
    "\n",
    "model.compile(optimizer = 'adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    Y_train_scaled, \n",
    "    validation_data=(X_test_scaled, Y_test_scaled),\n",
    "    epochs=1000,\n",
    "    batch_size=50,\n",
    "    verbose=0,\n",
    "    callbacks=[TqdmCallback(verbose=1)]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, mae = model.evaluate(X_test_scaled, Y_test_scaled, verbose=0)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")\n",
    "\n",
    "# Plot the loss over the Epochs\n",
    "plot_loss_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "Y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Reverse scale\n",
    "Y_pred = pd.DataFrame(\n",
    "    scaler_Y.inverse_transform(Y_pred_scaled),\n",
    "    index=Y_test.index,\n",
    "    columns=Y_test.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold over which the prediction is assessed as True\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "# Get the prediction errors for the one-hot encoded targets\n",
    "Y_pred_err = get_onehot_prediction_error(Y_pred, Y_test, prediction_threshold)\n",
    "\n",
    "# Plot the errors\n",
    "ax = Y_pred_err.droplevel(1, axis=1).T.plot(kind=\"bar\")\n",
    "ax.set_title(f\"Validation prediction with threshold {prediction_threshold}\")\n",
    "\n",
    "# Calculate the model metrics\n",
    "model_metrics = get_model_metrics(Y_test, Y_pred, prediction_threshold)\n",
    "models_metrics[\"LSTM\"] = model_metrics\n",
    "\n",
    "# Plot the model metrics\n",
    "model_metrics.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model from scratch - Simple Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df_features.astype(int),\n",
    "    df_targets.astype(int), \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_targets.values\n",
    ")\n",
    "\n",
    "if not np.all(Y_train.drop_duplicates().sum(axis=0) == 1):\n",
    "    raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# Scale data\n",
    "scaler_X = StandardScaler().fit(X_train.values)\n",
    "scaler_Y = StandardScaler().fit(Y_train.values)\n",
    "\n",
    "X_train_scaled = scaler_X.transform(X_train.values)\n",
    "X_test_scaled = scaler_X.transform(X_test.values)\n",
    "\n",
    "Y_train_scaled = scaler_Y.transform(Y_train.values)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test.values)\n",
    "\n",
    "print(f\"Training model to recognize {Y_train.shape[1]} target features.\\nUsing {X_train.shape[0]} samples with {X_train.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "# Define the feature inputs\n",
    "features_input = Input(shape=(X_train_scaled.shape[1],), name=\"features\")\n",
    "\n",
    "# Hidden layer\n",
    "x = Dense(64, activation=\"relu\", name=\"dense_1\")(features_input)\n",
    "x = Dense(32, activation=\"relu\", name=\"dense_2\")(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(Y_train_scaled.shape[1], activation=\"relu\", name=\"prediction\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[features_input],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "\n",
    "##  Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError]\n",
    ")\n",
    "\n",
    "##  Train the model\n",
    "history = model.fit(\n",
    "    [X_train_scaled],\n",
    "    [Y_train_scaled],\n",
    "    validation_split=0.1,\n",
    "    epochs=500,\n",
    "    verbose=0,\n",
    "    callbacks=[TqdmCallback(verbose=1)]\n",
    ")\n",
    "\n",
    "plot_loss_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([X_test_scaled], [Y_test_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model from scratch - Recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of sampled points\n",
    "n_points = 40\n",
    "\n",
    "# Time points, logspaced\n",
    "time_points = np.logspace(\n",
    "    np.log10(dat.index[0]),\n",
    "    np.log10(dat.index[-1]),\n",
    "    n_points\n",
    ")\n",
    "\n",
    "# Pre-populate the interp function\n",
    "_interp = partial(np.interp, time_points, dat.index)\n",
    "\n",
    "# Interpolate the selected points\n",
    "F_sampled = dat.apply(_interp)\n",
    "F_sampled.index = pd.MultiIndex.from_product([[\"Fsampled\"], time_points])\n",
    "\n",
    "# Add sampled points to features\n",
    "df_features = F_sampled.T\n",
    "\n",
    "# Subset the data to the samples and time to be included in the analysis \n",
    "ax = dat.plot(legend=False)\n",
    "\n",
    "for t in time_points:\n",
    "    ax.axvline(t)\n",
    "\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df_features.astype(int),\n",
    "    df_targets.astype(int), \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_targets.values\n",
    ")\n",
    "\n",
    "if not np.all(Y_train.drop_duplicates().sum(axis=0) == 1):\n",
    "    raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# Scale data\n",
    "scaler_X = StandardScaler().fit(X_train.values)\n",
    "scaler_Y = StandardScaler().fit(Y_train.values)\n",
    "\n",
    "X_train_scaled = scaler_X.transform(X_train.values)\n",
    "X_test_scaled = scaler_X.transform(X_test.values)\n",
    "\n",
    "Y_train_scaled = scaler_Y.transform(Y_train.values)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test.values)\n",
    "\n",
    "print(f\"Training model to recognize {Y_train.shape[1]} target features.\\nUsing {X_train.shape[0]} samples with {X_train.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "# Define the feature inputs\n",
    "ojip_input = Input(shape=(X_train_scaled.shape[1],), name=\"ojip_input\")\n",
    "\n",
    "x = keras.layers.Reshape((X_train_scaled.shape[1],1, name=\"LSTM_1_reshape\"))(ojip_input)\n",
    "\n",
    "# Hidden layer\n",
    "x = LSTM(64, activation=\"tanh\", name=\"LSTM_1\")(x)\n",
    "x = Dropout(0.3, name=\"LSTM_1_Dropout\")(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(Y_train_scaled.shape[1], activation=\"relu\", name=\"prediction\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[ojip_input],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "\n",
    "##  Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError]\n",
    ")\n",
    "\n",
    "##  Train the model\n",
    "history = model.fit(\n",
    "    [X_train_scaled],\n",
    "    [Y_train_scaled],\n",
    "    validation_split=0.1,\n",
    "    epochs=500,\n",
    "    verbose=0,\n",
    "    batch_size=10,\n",
    "    callbacks=[TqdmCallback(verbose=1)]\n",
    ")\n",
    "\n",
    "plot_loss_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(models_metrics), sharex=True)\n",
    "\n",
    "for model, ax in zip(models_metrics, axes.flatten()):\n",
    "    # Plot the model metrics\n",
    "    models_metrics[model].plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojipml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
