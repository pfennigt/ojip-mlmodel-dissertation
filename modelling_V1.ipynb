{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas.core.indexing import _IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.core.dtypes.common import is_numeric_dtype, is_object_dtype\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import common_functions as fnc\n",
    "\n",
    "idx: _IndexSlice = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import df, samples, didx, DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plots\n",
    "ax = df.loc[:, didx(\n",
    "    fluorometer=\"MULTI-COLOR-PAM\",\n",
    "    CO2_level=\"Air\", \n",
    "    strain=\"Chlorella vulgaris\",\n",
    "    # SP_color=455\n",
    ")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [Detector V]\")\n",
    "ax.set_title(\"MCPAM - Example\")\n",
    "\n",
    "ax = df.loc[:, didx(fluorometer=\"AquaPen\", CO2_level=\"Air\")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [AU]\")\n",
    "ax.set_title(\"AquaPen - Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the data to be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.loc[\n",
    "    0.01:, : # Exclude data before the light pulse\n",
    "    # didx(\n",
    "    #     fluorometer=\"MULTI-COLOR-PAM\", # Only use MCPAM data\n",
    "    #     strain='Synechocystis sp. PCC 6803', # Only use Synechocystis data\n",
    "    # )\n",
    "].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the map of effects transformed into one-hot encoding\n",
    "effects_map = pd.read_csv(\n",
    "    DATA_PATH / \"effects_map.csv\",\n",
    "    header=[0,1],\n",
    "    index_col=[0,1],\n",
    "    ).astype(float).fillna(0).astype(bool)\n",
    "\n",
    "# Exclude Light intensity and temperature from targets\n",
    "effects_map = effects_map[[\n",
    "    'control_measurement',\n",
    "    'PSII_closed',\n",
    "    'CBB_inhibited',\n",
    "    'TOX_inhibited',\n",
    "    'electron_drain'\n",
    "]]\n",
    "\n",
    "# Get the effects and map the mto the targets\n",
    "effects = samples.loc[dat.columns.get_level_values(0), [\"Effect in PSET\", \"Treatment\"]]\n",
    "\n",
    "targets = effects_map.loc[pd.MultiIndex.from_frame(effects)].droplevel(1, axis=1)\n",
    "targets.index = dat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the conditions as the Multiindex-columns\n",
    "conditions = dat.columns.to_frame()\n",
    "# conditions.index = dat.columns.get_level_values(0)\n",
    "\n",
    "# Select the relevant columns\n",
    "condition_map = pd.Series({\n",
    "    'Strain': \"str\",\n",
    "    'CO2 level': \"float\", # There is a meaning to a higher CO2 concentration (maybe make categorical?)\n",
    "    'Cultivation + experiment temperature': \"int\",\n",
    "    'Cultivation light intensity': \"int\",\n",
    "    'Dark or light acclimated': \"str\",\n",
    "    'Growth light color (nm)': \"str\",\n",
    "    'Fluorometer': \"str\",\n",
    "    'SP color (nm)': \"str\", # There is no linear relationship betweeen wavelength and effect\n",
    "    'SP intensity': \"int\",\n",
    "    'OD680 MC-1000': \"float\",\n",
    "    'OD720 MC-1000': \"float\",\n",
    "})\n",
    "\n",
    "conditions = conditions[condition_map.index]\n",
    "\n",
    "# Replace certain column values\n",
    "\n",
    "# Replace CO2 level with the actual (assumed) numerical ppm\n",
    "conditions[\"CO2 level\"] = conditions[\"CO2 level\"].replace({\n",
    "    \"Air\": \"0.0004\",\n",
    "    \"High CO2\": \"0.05\"\n",
    "}).astype(float)\n",
    "\n",
    "# Replace SP color with categorical value because the numerical gradient is not meaningful\n",
    "conditions[\"SP color (nm)\"] = conditions[\"SP color (nm)\"].astype(str)\n",
    "\n",
    "# Encode conditions in one-hot\n",
    "categorical_conditions = condition_map[condition_map == \"str\"].index.to_numpy()\n",
    "numerical_conditions = condition_map[condition_map != \"str\"].index.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all information layers\n",
    "dat_full = pd.concat({\n",
    "    'conditions': conditions,\n",
    "    \"targets\": targets,\n",
    "    # \"ojip\":ojip,\n",
    "    }, \n",
    "    names=['type'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  df = dataframe.copy()\n",
    "\n",
    "  targets = df[\"targets\"]\n",
    "  conditions = df[\"conditions\"]\n",
    "\n",
    "\n",
    "  targets = {key: value.to_numpy()[:,tf.newaxis] for key, value in targets.items()}\n",
    "  conditions = {key: value.to_numpy()[:,tf.newaxis] for key, value in conditions.items()}\n",
    "  \n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(conditions), dict(targets)))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into dataset\n",
    "test = df_to_dataset(dat_full, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.as_numpy_iterator():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample OJIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of sampled points\n",
    "n_points = 40\n",
    "\n",
    "# Time points, logspaced\n",
    "time_points = np.logspace(\n",
    "    np.log10(dat.index[0]),\n",
    "    np.log10(dat.index[-1]),\n",
    "    n_points\n",
    ")\n",
    "\n",
    "# Pre-populate the interp function\n",
    "_interp = partial(np.interp, time_points, dat.index)\n",
    "\n",
    "# Interpolate the selected points\n",
    "ojip_sampled = dat.apply(_interp)\n",
    "ojip_sampled.index = pd.MultiIndex.from_product([[\"Fsampled\"], time_points])\n",
    "\n",
    "# Add sampled points to features\n",
    "ojip_sampled = ojip_sampled.T\n",
    "\n",
    "# Subset the data to the samples and time to be included in the analysis \n",
    "ax = dat.plot(legend=False)\n",
    "\n",
    "for t in time_points:\n",
    "    ax.axvline(t)\n",
    "\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for UMAP\n",
    "UMAP_seed = 2025\n",
    "\n",
    "# Scale the features\n",
    "df_features_scaled = StandardScaler().fit_transform(df_features.values)\n",
    "\n",
    "# Create the UMAP embedding\n",
    "reducer = umap.UMAP(random_state=UMAP_seed)\n",
    "embedding = pd.DataFrame(\n",
    "    reducer.fit_transform(df_features_scaled),\n",
    "    index=df_features.index,\n",
    "    columns=[\"UMAP_1\", \"UMAP_2\"]\n",
    ").reset_index()\n",
    "\n",
    "# Plot\n",
    "categories = df.columns.names[1:]\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,15),\n",
    "    sharey=True,\n",
    "    sharex=True,\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot targest on UMAP\n",
    "# Add UMAP to targets\n",
    "embedding_targets = pd.concat([\n",
    "    targets.droplevel(-1, axis=1).droplevel(list(range(1,20)), axis=0),\n",
    "    embedding.set_index(\"Label\").loc[:, [\"UMAP_1\", \"UMAP_2\"]],\n",
    "], axis=1)\n",
    "\n",
    "# Plot\n",
    "categories = effects_map.columns.get_level_values(0)\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,7),\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding_targets,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding_targets[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "#     df_features.astype(int),\n",
    "#     targets.astype(int), \n",
    "#     test_size=0.2, \n",
    "#     random_state=42,\n",
    "#     stratify=targets.values\n",
    "# )\n",
    "\n",
    "# if not np.all(Y_train.drop_duplicates().sum(axis=0) == 1):\n",
    "#     raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# # Scale data\n",
    "# scaler_X = StandardScaler().fit(X_train.values)\n",
    "# scaler_Y = StandardScaler().fit(Y_train.values)\n",
    "\n",
    "# X_train_scaled = scaler_X.transform(X_train.values)\n",
    "# X_test_scaled = scaler_X.transform(X_test.values)\n",
    "\n",
    "# Y_train_scaled = scaler_Y.transform(Y_train.values)\n",
    "# Y_test_scaled = scaler_Y.transform(Y_test.values)\n",
    "\n",
    "# print(f\"Training model to recognize {Y_train.shape[1]} target features.\\nUsing {X_train.shape[0]} samples with {X_train.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "# Define the feature inputs\n",
    "ojip_input = Input(shape=(X_train_scaled.shape[1],), name=\"ojip_input\")\n",
    "\n",
    "x = keras.layers.Reshape((X_train_scaled.shape[1],1), name=\"LSTM_1_reshape\")(ojip_input)\n",
    "\n",
    "# Hidden layer\n",
    "x = LSTM(64, activation=\"tanh\", name=\"LSTM_1\")(x)\n",
    "x = Dropout(0.3, name=\"LSTM_1_Dropout\")(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(Y_train_scaled.shape[1], activation=\"relu\", name=\"prediction\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[ojip_input],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "\n",
    "##  Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError]\n",
    ")\n",
    "\n",
    "##  Train the model\n",
    "history = model.fit(\n",
    "    [X_train_scaled],\n",
    "    [Y_train_scaled],\n",
    "    validation_split=0.1,\n",
    "    epochs=500,\n",
    "    verbose=0,\n",
    "    batch_size=10,\n",
    "    callbacks=[TqdmCallback(verbose=1)]\n",
    ")\n",
    "\n",
    "plot_loss_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(models_metrics), sharex=True)\n",
    "\n",
    "for model, ax in zip(models_metrics, axes.flatten()):\n",
    "    # Plot the model metrics\n",
    "    models_metrics[model].plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojipml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
