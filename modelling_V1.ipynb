{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas.core.indexing import _IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.core.dtypes.common import is_numeric_dtype, is_object_dtype\n",
    "import datetime\n",
    "\n",
    "import itertools\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import common_functions as fnc\n",
    "\n",
    "idx: _IndexSlice = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import df, samples, didx, DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plots\n",
    "ax = df.loc[:, didx(\n",
    "    fluorometer=\"MULTI-COLOR-PAM\",\n",
    "    CO2_level=\"Air\", \n",
    "    strain=\"Chlorella vulgaris\",\n",
    "    # SP_color=455\n",
    ")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [Detector V]\")\n",
    "ax.set_title(\"MCPAM - Example\")\n",
    "\n",
    "ax = df.loc[:, didx(fluorometer=\"AquaPen\", CO2_level=\"Air\")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [AU]\")\n",
    "ax.set_title(\"AquaPen - Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the data to be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.loc[\n",
    "    0.01:, : # Exclude data before the light pulse\n",
    "    # didx(\n",
    "    #     fluorometer=\"MULTI-COLOR-PAM\", # Only use MCPAM data\n",
    "    #     strain='Synechocystis sp. PCC 6803', # Only use Synechocystis data\n",
    "    # )\n",
    "].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = pd.read_csv(\n",
    "    DATA_PATH / \"effects_map.csv\",\n",
    "    header=[0,1],\n",
    "    index_col=[0,1],\n",
    "    ).astype(float).fillna(0).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the map of effects transformed into one-hot encoding\n",
    "effects_map_raw = pd.read_csv(\n",
    "    DATA_PATH / \"effects_map.csv\",\n",
    "    header=[0,1],\n",
    "    index_col=[0,1],\n",
    "    ).astype(float).fillna(0).astype(bool)\n",
    "\n",
    "# Exclude Light intensity and temperature from targets\n",
    "effects_map = effects_map_raw[[\n",
    "    'control_measurement',\n",
    "    'PSII_closed',\n",
    "    'CBB_inhibited',\n",
    "    'TOX_inhibited',\n",
    "    'electron_drain'\n",
    "]]\n",
    "\n",
    "# Set High light as a closing factor of PSII\n",
    "effects_map.loc[:,'PSII_closed'] = np.logical_or(\n",
    "    effects_map['PSII_closed'].to_numpy(),\n",
    "    effects_map_raw[\"high_light\"].to_numpy()\n",
    ")\n",
    "\n",
    "# Ass high and low temperature as \n",
    "effects_map.loc[:,'control_measurement'] = np.logical_or(\n",
    "    effects_map['control_measurement'].to_numpy(),\n",
    "    effects_map_raw[\"low_temperature\"].to_numpy()\n",
    ")\n",
    "effects_map.loc[:,'control_measurement'] = np.logical_or(\n",
    "    effects_map['control_measurement'].to_numpy(),\n",
    "    effects_map_raw[\"high_temperature\"].to_numpy()\n",
    ")\n",
    "effects_map.loc[:,'control_measurement'] = np.logical_or(\n",
    "    effects_map['control_measurement'].to_numpy(),\n",
    "    effects_map_raw[\"low_light\"].to_numpy()\n",
    ")\n",
    "\n",
    "# Get the effects and map the mto the targets\n",
    "effects = samples.loc[dat.columns.get_level_values(0), [\"Effect in PSET\", \"Treatment\"]]\n",
    "\n",
    "targets = effects_map.loc[pd.MultiIndex.from_frame(effects)].droplevel(1, axis=1)\n",
    "targets = targets.astype(int)\n",
    "targets.index = dat.columns\n",
    "\n",
    "# Make a Multiindex with a duplicated entries\n",
    "target_names = targets.columns\n",
    "targets.columns = pd.MultiIndex.from_arrays(arrays=[targets.columns, targets.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Message from Tomas:**\n",
    "\n",
    "INPUTS\n",
    "- Strain: Synechocystis / Chlorella. Later, when we have more strains measured, we can make it more general like green algae/cyanobacteria.\n",
    "- Temperature: **it could be enough to keep it in intervals, like e.g. 15-20 / 20-25 / 25-30 / 30-35 etc.**\n",
    "- Light acclimation state: light-acclimated / dark-acclimated\n",
    "- OD: needs to be standardized = measured by one device since each device measure OD differently.  In the dataset we now have it for Multi-Cultivator, but it would be better to refer to UV-Vis and 1 cm cuvette. Alternatively, chlorophyll content could work well. I can provide all these values, based on additional measurements that we have already performed.\n",
    "- SP color: **blue / orange-red could be enough**\n",
    "- SP Intensity: saturating / non-saturating. This parameter definitely matters, but it could be tricky for the users to validate if SP is saturating or not. So, alternatively, the user can provide SP intensity (**perhaps again in some intervals like < 1000 / 1000-1500 / 1500-2000 / 2000-2500 / >2500**) and we can evaluate by our algorithm if the SP was saturating, based on the selection of strain, OD, SP color and SP intensity.\n",
    "- Fluorometer: MCPAM / AquaPen - I would keep it for now, since I am still not sure if there is any fundamental difference between MC-PAM, AquaPen and FL-6000, or not. But in general, it should be only SP color and SP intensity that matter the most\n",
    "- These parameters might be not necessary, but I am not sure - the OJIP curves were different under these specific acclimations, so it could make the predictions better if we discriminate also these acclimations - but on the other hand, it might be overkill:\n",
    "  - CO2 level\n",
    "  - Growth light color\n",
    "\n",
    "OUTPUTS\n",
    "- PSII closed: DCMU, high light\n",
    "- CBB inhibited: Glycolaldehyde\n",
    "- TOX inhibited: KCN\n",
    "- Electron drain from PSI: Methylviologen. Btw. yes, this effect should be the opposite of Glycolaldehyde treatment\n",
    "- **PQ redox state: reduced / oxidized**\n",
    "- It would be great to include more inhibitors, to describe more effects -we can do that later:\n",
    "  - Cyt b6/f closed: DBMIB\n",
    "  - Cyclic electron flow around PSI closed: Antimycin A3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the conditions as the Multiindex-columns\n",
    "conditions = dat.columns.to_frame()\n",
    "# conditions.index = dat.columns.get_level_values(0)\n",
    "\n",
    "# Select the relevant columns\n",
    "condition_types = pd.Series({\n",
    "    'Strain': \"string\",\n",
    "    'CO2 level': \"string\", # There is a meaning to a higher CO2 concentration (maybe make categorical?)\n",
    "    'Cultivation + experiment temperature': (\"bucket\", [15, 20, 25, 30, 35]),\n",
    "    'Cultivation light intensity': \"numeric\",\n",
    "    'Dark or light acclimated': \"string\",\n",
    "    'Growth light color (nm)': \"string\",\n",
    "    'Fluorometer': \"string\",\n",
    "    'SP color (nm)': \"int64\", # There is no linear relationship between wavelength and effect\n",
    "    'SP intensity': (\"bucket\", [1000, 1500, 2000, 2500]),\n",
    "    'OD680 MC-1000': \"numeric\",\n",
    "    'OD720 MC-1000': \"numeric\",\n",
    "})\n",
    "\n",
    "conditions = conditions[condition_types.index]\n",
    "\n",
    "# Make a Multiindex with a duplicated entries\n",
    "conditions.columns = pd.MultiIndex.from_arrays([conditions.columns, conditions.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample OJIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.index[-1\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of sampled points\n",
    "n_points = 40\n",
    "\n",
    "# Time points, logspaced\n",
    "log_time_points = np.linspace(\n",
    "    np.log10(dat.index[0]),\n",
    "    np.log10(dat.index[-1]),\n",
    "    n_points\n",
    ")\n",
    "time_points = 10 ** log_time_points\n",
    "\n",
    "# Pre-populate the interp function\n",
    "_interp = partial(np.interp, time_points, dat.index)\n",
    "\n",
    "# Interpolate the selected points\n",
    "ojip_sampled = dat.apply(_interp)\n",
    "ojip_sampled.index = pd.MultiIndex.from_product([\n",
    "    [\"ojip\"],\n",
    "    [\"ojip_\" + x for x in log_time_points.round(2).astype(str)]\n",
    "])\n",
    "\n",
    "# Add sampled points to features\n",
    "ojip_sampled = ojip_sampled.T\n",
    "\n",
    "# Subset the data to the samples and time to be included in the analysis \n",
    "ax = dat.plot(legend=False)\n",
    "\n",
    "for t in time_points:\n",
    "    ax.axvline(t, c=\"k\", alpha=0.5)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Add data types\n",
    "ojip_types = pd.Series({\"ojip\":\"time-series-gradients\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all data sets\n",
    "dat_sets = [\n",
    "    conditions,\n",
    "    ojip_sampled,\n",
    "    targets\n",
    "]\n",
    "dat_full = pd.concat(dat_sets, axis=1)\n",
    "\n",
    "# Split data into training set\n",
    "dat_train, _dat_trainval = train_test_split(\n",
    "    dat_full,\n",
    "    test_size=0.2, \n",
    "    random_state=SEED,\n",
    "    stratify=dat_full[target_names]\n",
    ")\n",
    "\n",
    "# Split data into test and validation set\n",
    "dat_test, dat_val = train_test_split(\n",
    "    _dat_trainval,\n",
    "    test_size=0.5, \n",
    "    random_state=SEED,\n",
    "    stratify=_dat_trainval[target_names]\n",
    ")\n",
    "\n",
    "print(f\"Dimensions train: {dat_train.shape}, test: {dat_test.shape}, val: {dat_val.shape}\")\n",
    "\n",
    "\n",
    "if not np.all(dat_train[target_names].drop_duplicates().sum(axis=0) == 1):\n",
    "    raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# Make into dataset\n",
    "train_ds = fnc.df_to_dataset(dat_train, targets=target_names, batch_size=64, shuffle=True)\n",
    "test_ds = fnc.df_to_dataset(dat_test, targets=target_names, shuffle=False)\n",
    "val_ds = fnc.df_to_dataset(dat_val, targets=target_names, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the types of all features\n",
    "feature_types = pd.concat([\n",
    "    condition_types,\n",
    "    ojip_types\n",
    "])\n",
    "\n",
    "# Create containers for inputs and encodings\n",
    "all_inputs = {}\n",
    "encoded_features_dict = {}\n",
    "encoded_features = []\n",
    "\n",
    "# Encode all features\n",
    "for col_name, col_dtype in feature_types.items():\n",
    "\n",
    "    if isinstance(col_dtype, str):\n",
    "        # Create a numeric normalisation layer\n",
    "        if col_dtype == \"numeric\":\n",
    "            col = layers.Input(shape=(1,), name=col_name)\n",
    "            normalization_layer = fnc.get_normalization_layer(col_name, train_ds)\n",
    "            encoded_col = normalization_layer(col)\n",
    "        \n",
    "        # Create a string enconding layer, could also work for integer encoding\n",
    "        elif col_dtype in [\"string\", \"int64\"]:\n",
    "            col = layers.Input(shape=(1,), name=col_name, dtype=col_dtype)\n",
    "            encoding_layer = fnc.get_category_encoding_layer(name=col_name,\n",
    "                                                        dataset=train_ds,\n",
    "                                                        dtype=col_dtype,\n",
    "                                                        max_tokens=5)\n",
    "            encoded_col = encoding_layer(col)\n",
    "\n",
    "        # Create a layer to normalise time series and calculate gradients\n",
    "        elif col_dtype == \"time-series-gradients\":\n",
    "            col = layers.Input(shape=(dat_full[col_name].shape[1],), name=col_name)\n",
    "            reshaped_col = layers.Reshape((dat_full[col_name].shape[1], 1))(col)\n",
    "            normalization_layer = fnc.NormalizedTimeSeriesWithDerivatives()\n",
    "            encoded_col = normalization_layer(reshaped_col)\n",
    "            \n",
    "        else:\n",
    "            raise KeyError(f\"No handling for col_dtype {col_dtype} defined\")\n",
    "\n",
    "    elif isinstance(col_dtype, tuple):\n",
    "        # Create a discretized encoding for values with non-linear behaviour\n",
    "        if col_dtype[0] == \"bucket\":\n",
    "            col = layers.Input(shape=(1,), name=col_name, dtype=\"float32\")\n",
    "            encoding_layer = fnc.get_bucket_encoding_layer(name=col_name,\n",
    "                                                        bin_boundaries=col_dtype[1])\n",
    "            encoded_col = encoding_layer(col)\n",
    "        else:\n",
    "            raise KeyError(f\"No handling for col_dtype {col_dtype[0]} defined\")\n",
    "    else:\n",
    "        raise KeyError(f\"No handling for col_dtype {col_dtype} defined\")\n",
    "\n",
    "    all_inputs[col_name] = col\n",
    "    encoded_features.append(encoded_col)\n",
    "    encoded_features_dict[col_name] = encoded_col\n",
    "\n",
    "# Define the preprocessing layer as a model\n",
    "preprocessing_layer = keras.Model(\n",
    "    all_inputs,\n",
    "    encoded_features_dict,\n",
    "    name=\"preprocessing_layer\"\n",
    ")\n",
    "\n",
    "# Plot the preprocessing layer\n",
    "keras.utils.plot_model(\n",
    "    preprocessing_layer,\n",
    "    # show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"LR\",\n",
    "    to_file=\"figures/preprocessing.png\",\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the condition combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _discretize_ranges(_range , _index, _all_index, conditions):\n",
    "#     if _range[0] is None:\n",
    "#         conditions[_index] = conditions[_all_index] < _range[1]\n",
    "#     elif _range[1] is None:\n",
    "#         conditions[_index] = conditions[_all_index] > _range[0]\n",
    "#     else:\n",
    "#         conditions[_index] = np.logical_and(\n",
    "#             conditions[_all_index] >= _range[0],\n",
    "#             conditions[_all_index] < _range[1],\n",
    "#         )\n",
    "#     return conditions\n",
    "\n",
    "def _discretize_ranges(_range , value, _all_index, conditions):\n",
    "    if _range[0] is None:\n",
    "        _range_bool = conditions[_all_index] < _range[1]\n",
    "    elif _range[1] is None:\n",
    "        _range_bool = conditions[_all_index] >= _range[0]\n",
    "    else:\n",
    "        _range_bool = np.logical_and(\n",
    "            conditions[_all_index] >= _range[0],\n",
    "            conditions[_all_index] < _range[1],\n",
    "        )\n",
    "    conditions.loc[_range_bool, (f\"_{_all_index[0]}\", _all_index[1])] = value\n",
    "    return conditions\n",
    "\n",
    "\n",
    "def discretize_ranges(conditions, condition_name, ranges):\n",
    "    # Encode temperature in ranges as one-hot\n",
    "    _all_index = (condition_name, condition_name)\n",
    "    conditions[(f\"_{condition_name}\", condition_name)] = conditions[_all_index].astype(str)\n",
    "    for nam, _range in ranges.items():\n",
    "        conditions = _discretize_ranges(_range , nam, _all_index, conditions)\n",
    "\n",
    "    conditions[_all_index] = conditions[(f\"_{condition_name}\", condition_name)]\n",
    "    conditions=conditions.drop((f\"_{condition_name}\", condition_name), axis=1)\n",
    "    return conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique levels for selected conditions\n",
    "conditions_table = conditions[[\n",
    "    \"Strain\",\n",
    "    \"CO2 level\",\n",
    "    \"Cultivation + experiment temperature\",\n",
    "    \"Cultivation light intensity\",\n",
    "    \"Dark or light acclimated\",\n",
    "    # \"Fluorometer\",\n",
    "    # \"Growth light color (nm)\",\n",
    "    \"SP color (nm)\",\n",
    "    \"SP intensity\",\n",
    "]].copy()\n",
    "\n",
    "# Add the type of treatment\n",
    "conditions_table[\"Treatment type\"] = [effects_map[target_names].columns.get_level_values(1).to_numpy()[x][0] for x in targets.astype(bool).to_numpy()]\n",
    "\n",
    "# Get all unique entries per columns type\n",
    "unique_conditions = conditions_table.apply(np.unique)\n",
    "\n",
    "for col_name, col_dtype in condition_types.items():\n",
    "    if isinstance(col_dtype,tuple) and col_dtype[0]==\"bucket\":\n",
    "        _ranges = [None] + col_dtype[1] + [None]\n",
    "        ranges = {f\"{_ranges[i]} - {_ranges[i+1]}\":_ranges[i:i+2] for i in range(len(_ranges)-1)}\n",
    "        conditions_table = discretize_ranges(conditions_table, col_name, ranges)\n",
    "\n",
    "        # Add all buckets to the unique values\n",
    "        unique_conditions[(col_name, col_name)] = list(ranges.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Add the type of treatment\n",
    "# conditions_table[\"Treatment type\"] = [effects_map[target_names].columns.get_level_values(1).to_numpy()[x][0] for x in targets.astype(bool).to_numpy()]\n",
    "\n",
    "# # Get all unique entries per columns type\n",
    "# unique_conditions = conditions_table.apply(np.unique)\n",
    "\n",
    "# Create all possible conditions combinations\n",
    "all_condition_counts = pd.Series(\n",
    "    data=0, \n",
    "    index=pd.MultiIndex.from_tuples(\n",
    "        list(itertools.product(*unique_conditions.to_numpy())),\n",
    "        names=conditions_table.columns.get_level_values(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create all possible conditions combinations\n",
    "all_condition_counts = pd.Series(\n",
    "    data=0, \n",
    "    index=pd.MultiIndex.from_tuples(\n",
    "        list(itertools.product(*unique_conditions.to_numpy())),\n",
    "        names=conditions_table.columns.get_level_values(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count the observations of each condition combination and place the min the total table\n",
    "condition_counts = pd.Series(dict(Counter([tuple(x) for x in conditions_table.to_numpy()])))\n",
    "\n",
    "all_condition_counts.update(condition_counts)\n",
    "\n",
    "all_condition_counts = pd.concat([\n",
    "    all_condition_counts.loc[all_condition_counts>0].sort_values(ascending=False),\n",
    "    all_condition_counts.loc[all_condition_counts==0].sort_index()\n",
    "])\n",
    "\n",
    "all_condition_counts.name = \"Sample counts\"\n",
    "\n",
    "all_condition_counts.to_csv(\"out/ojip_sample_counts.csv\")\n",
    "print(f\"number of cases not covered: {(all_condition_counts==0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_conditions.droplevel(1).to_csv(\"out/ojip_conditions_unique.csv\")\n",
    "unique_conditions.droplevel(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create UMAP mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the outputs of the preprocessing layer to perform UMAP\n",
    "preprocessed = preprocessing_layer(all_inputs)\n",
    "\n",
    "# Append the derivatives of the ojip signal to the end to create a long feature vector\n",
    "preprocessed[\"ojip\"] = layers.Flatten()(preprocessed[\"ojip\"])\n",
    "\n",
    "concatenated_preprocess = keras.Model(\n",
    "    all_inputs,\n",
    "    layers.concatenate(list(preprocessed.values())),\n",
    "    name=\"concatenation_layer\"\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "df_features_scaled = concatenated_preprocess.predict(\n",
    "    fnc.get_dataset_from_input_df(dat_full, all_inputs)\n",
    ")\n",
    "\n",
    "# Create the UMAP embedding\n",
    "reducer = umap.UMAP(random_state=SEED)\n",
    "embedding = pd.DataFrame(\n",
    "    reducer.fit_transform(df_features_scaled),\n",
    "    index=dat_full.index,\n",
    "    columns=[\"UMAP_1\", \"UMAP_2\"]\n",
    ").reset_index()\n",
    "\n",
    "# Plot\n",
    "categories = df.columns.names[1:]\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,15),\n",
    "    sharey=True,\n",
    "    sharex=True,\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/umap_conditions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot targest on UMAP\n",
    "# Add UMAP to targets\n",
    "embedding_targets = pd.concat([\n",
    "    targets.droplevel(-1, axis=1).droplevel(list(range(1,20)), axis=0),\n",
    "    embedding.set_index(\"Label\").loc[:, [\"UMAP_1\", \"UMAP_2\"]],\n",
    "], axis=1)\n",
    "\n",
    "# Plot\n",
    "categories = effects_map.columns.get_level_values(0)\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,5),\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding_targets,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding_targets[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/umap_treatments.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_output_to_dict(dense_output):\n",
    "    outputs = {\n",
    "        \"control_measurement\": layers.Lambda(lambda x: tf.expand_dims(x[:, 0], axis=-1), name=\"control_measurement\")(dense_output),\n",
    "        \"PSII_closed\": layers.Lambda(lambda x: tf.expand_dims(x[:, 1], axis=-1), name=\"PSII_closed\")(dense_output),\n",
    "        \"CBB_inhibited\": layers.Lambda(lambda x: tf.expand_dims(x[:, 2], axis=-1), name=\"CBB_inhibited\")(dense_output),\n",
    "        \"TOX_inhibited\": layers.Lambda(lambda x: tf.expand_dims(x[:, 3], axis=-1), name=\"TOX_inhibited\")(dense_output),\n",
    "        \"electron_drain\": layers.Lambda(lambda x: tf.expand_dims(x[:, 4], axis=-1), name=\"electron_drain\")(dense_output),\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model\n",
    "# Get the preprocessed inputs\n",
    "preprocessed = preprocessing_layer(all_inputs)\n",
    "\n",
    "# Append the derivatives of the ojip signal to the end to create a long feature vector\n",
    "preprocessed[\"ojip\"] = layers.Flatten()(preprocessed[\"ojip\"])\n",
    "\n",
    "# Concatenate all features\n",
    "x = layers.concatenate(list(preprocessed.values()))\n",
    "\n",
    "# Dense hidden layer\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Dense layer for output calculation\n",
    "# Uses sigmoid activation function for output between 0 and 1\n",
    "dense_output = layers.Dense(targets.shape[1], activation=\"sigmoid\")(x)\n",
    "\n",
    "# Split the output into a dictionary\n",
    "outputs = split_output_to_dict(dense_output)\n",
    "\n",
    "\n",
    "# Create and compile test model\n",
    "test_model = keras.Model(all_inputs, outputs)\n",
    "test_model.compile(\n",
    "    loss='binary_crossentropy', # Expects output to be multiple probabilities for classes\n",
    "    optimizer='adam',\n",
    "    metrics={key:[ # Calculate metrics for each target\n",
    "        keras.metrics.BinaryAccuracy(threshold=0.5, name=\"..binary_accuracy\"), # Accuracy (assumes threshold 0.5)\n",
    "        keras.metrics.Recall(thresholds=0.5, name=\"..recall\"), # Recall (assumes threshold 0.5)\n",
    "        keras.metrics.Precision(thresholds=0.5, name=\"..precision\"), # Precision (assumes threshold 0.5)\n",
    "        keras.metrics.F1Score(threshold=0.5, name=\"..f1_score\"), # F1-score (assumes threshold 0.5)\n",
    "        ] for key in target_names},\n",
    ")\n",
    "\n",
    "# Plot the model\n",
    "keras.utils.plot_model(\n",
    "    test_model,\n",
    "    # show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    to_file=\"figures/dense_model.png\",\n",
    "    rankdir=\"LR\",\n",
    "    # expand_nested=True,\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Use tensorboard to track the training\n",
    "use_tensorboard = False\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = [keras.callbacks.TensorBoard(log_dir=log_dir)] if use_tensorboard else []\n",
    "\n",
    "test_model_history = test_model.fit(\n",
    "    train_ds,\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[\n",
    "        TqdmCallback(verbose=1)\n",
    "    ] + tensorboard_callback\n",
    ")\n",
    "\n",
    "# Plot the loss over the Epochs\n",
    "fig, ax = fnc.plot_loss_development(test_model_history)\n",
    "fig.savefig(\"figures/dense_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_model_eval = test_model.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "fig, axes = fnc.plot_model_metrics(test_model_eval, test_ds)\n",
    "fig.savefig(\"figures/dense_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_eval_train = test_model.evaluate(train_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_eval_compare = {x:(test_model_eval_train[x] - test_model_eval[x]) for x in test_model_eval}\n",
    "test_model_eval_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = fnc.plot_model_metrics(test_model_eval_compare, test_ds)\n",
    "fig.suptitle(\"Increase in metric for training data\", y=1, weight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplary prediction\n",
    "fnc.predict_model_from_df(test_model, dat_test).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "# Get the preprocessed inputs\n",
    "seperated_inputs = preprocessing_layer(all_inputs)\n",
    "\n",
    "# Add an LSTM layer to the OJIP input\n",
    "seperated_inputs[\"ojip\"] = layers.LSTM(16, activation=\"tanh\", name=\"LSTM_ojip\")(seperated_inputs[\"ojip\"])\n",
    "seperated_inputs[\"ojip\"] = layers.Dropout(0.5, name=\"LSTM_1_Dropout\")(seperated_inputs[\"ojip\"])\n",
    "\n",
    "# Concatenate all features\n",
    "x = layers.concatenate(list(seperated_inputs.values()))\n",
    "\n",
    "# Dense hidden layer\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Dense layer for output calculation\n",
    "# Uses sigmoid activation function for output between 0 and 1\n",
    "dense_output = layers.Dense(targets.shape[1], activation=\"sigmoid\")(x)\n",
    "\n",
    "# Split the output into a dictionary\n",
    "outputs = split_output_to_dict(dense_output)\n",
    "\n",
    "# Create and compile test model\n",
    "model = keras.Model(all_inputs, outputs)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', # Expects output to be multiple probabilities for classes\n",
    "    optimizer='adam',\n",
    "    metrics={key:[ # Calculate metrics for each target\n",
    "        keras.metrics.BinaryAccuracy(threshold=0.5, name=\"..binary_accuracy\"), # Accuracy (assumes threshold 0.5)\n",
    "        keras.metrics.Recall(thresholds=0.5, name=\"..recall\"), # Recall (assumes threshold 0.5)\n",
    "        keras.metrics.Precision(thresholds=0.5, name=\"..precision\"), # Precision (assumes threshold 0.5)\n",
    "        keras.metrics.F1Score(threshold=0.5, name=\"..f1_score\"), # F1-score (assumes threshold 0.5)\n",
    "        ] for key in target_names},\n",
    ")\n",
    "\n",
    "# Plot the model\n",
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    # show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    to_file=\"figures/lstm_model.png\",\n",
    "    rankdir=\"LR\",\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Use tensorboard to track the training\n",
    "use_tensorboard = False\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = [keras.callbacks.TensorBoard(log_dir=log_dir)] if use_tensorboard else []\n",
    "\n",
    "model_history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=800,\n",
    "    verbose=0,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[\n",
    "        TqdmCallback(verbose=1)\n",
    "    ] + tensorboard_callback\n",
    ")\n",
    "\n",
    "# Plot the loss over the Epochs\n",
    "fig, ax = fnc.plot_loss_development(model_history)\n",
    "fig.savefig(\"figures/lstm_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final metrics of a model\n",
    "def plot_history_metrics(history, fit_dataset, val_dataset):\n",
    "    # Get the targets from the dataset to count occurrences\n",
    "    target_numbers={}\n",
    "    for nam, ds in {\"fit\":fit_dataset, \"val\":val_dataset}.items():\n",
    "        _targets_subsets = list(ds.map(lambda x,y: y).as_numpy_iterator())\n",
    "        _targets = [pd.DataFrame({k:v.flatten() for k,v in _targets_subset.items()}) for _targets_subset in _targets_subsets]\n",
    "        _targets = pd.concat(_targets, axis=0)\n",
    "\n",
    "        target_numbers[nam] = _targets.sum().sort_values(ascending=False)\n",
    "\n",
    "    metrics = history.history\n",
    "\n",
    "    # Get all metrics and assign them to the targets\n",
    "    plot_metrics = pd.DataFrame(\n",
    "        {tuple(k.split(\"_..\")): v for k,v in metrics.items() if not k.endswith(\"loss\")}\n",
    "    )\n",
    "\n",
    "    print(plot_metrics.columns.levels[0])\n",
    "\n",
    "    metrics_names = plot_metrics.columns.levels[1]\n",
    "    targets_names = _targets.columns\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axes = plt.subplots(\n",
    "        len(targets_names), \n",
    "        2,\n",
    "        figsize=(7,7),\n",
    "        sharex=True,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    # Plot each target\n",
    "    for j, val in enumerate([\"fit\", \"val\"]):\n",
    "        for i, _target in enumerate(targets_names):\n",
    "            prefix = \"val_\" if val==\"val\" else \"\"\n",
    "            target = prefix + _target\n",
    "            axes[i, j].plot(\n",
    "                plot_metrics.loc[:, idx[target, metrics_names]],\n",
    "                label=metrics_names\n",
    "            )\n",
    "            axes[i, j].set_title(f\"{target} (n={target_numbers[val][_target]})\")\n",
    "\n",
    "        axes[-1, j].set_xlabel(\"Epoch No.\")\n",
    "\n",
    "    for ax in axes[:,0]:\n",
    "        ax.set_ylabel(\"Metric\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    axes[0, -1].legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_history_metrics(history=model_history, fit_dataset=train_ds, val_dataset=val_ds)\n",
    "for ax in axes.flatten():\n",
    "    ax.set_ylim(0,1)\n",
    "fig.savefig(\"figures/lstm_metrics_over_time.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "model_eval = model.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "fig, axes = fnc.plot_model_metrics(model_eval, test_ds, ylim=(0, 1))\n",
    "fig.savefig(\"figures/lstm_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final metrics of a model\n",
    "def plot_model_metrics_multi(metrics_dict:dict, dataset, ylim=(0,1), width_scale=0.5):\n",
    "    for m, model in enumerate(metrics_dict):\n",
    "        metrics = metrics_dict[model]\n",
    "        # Get the targets from the dataset to count occurrences\n",
    "        _targets_subsets = list(dataset.map(lambda x,y: y).as_numpy_iterator())\n",
    "        _targets = [pd.DataFrame({k:v.flatten() for k,v in _targets_subset.items()}) for _targets_subset in _targets_subsets]\n",
    "        _targets = pd.concat(_targets, axis=0)\n",
    "        target_number = _targets.sum().sort_values(ascending=False)\n",
    "\n",
    "        # Get all metrics and assign them to the targets\n",
    "        plot_metrics = pd.Series(\n",
    "            {tuple(k.split(\"_..\")): v for k,v in metrics.items() if not k.endswith(\"loss\")}\n",
    "        )\n",
    "        metrics_names = plot_metrics.index.levels[1]\n",
    "\n",
    "\n",
    "        # Create the figure\n",
    "        if m==0:\n",
    "            fig, axes = plt.subplots(\n",
    "                len(metrics_names) + 1, \n",
    "                1,\n",
    "                figsize=(7,7),\n",
    "                sharex=True\n",
    "            )\n",
    "            offset = 1/len(metrics_dict) * width_scale\n",
    "            width = 1/(len(metrics_dict)) * width_scale\n",
    "\n",
    "        # Plot the number of occurrences\n",
    "        axes[0].bar(\n",
    "            range(len(target_number)),\n",
    "            target_number,\n",
    "            label=model\n",
    "        )\n",
    "        axes[0].set_title(\"Number of samples\")\n",
    "        axes[0].set_ylabel(\"Number of samples\")\n",
    "\n",
    "        for i, metric in enumerate(metrics_names):\n",
    "            axes[i+1].bar(\n",
    "                np.arange(len(target_number)) - (0.25 * width_scale) + m * offset,\n",
    "                plot_metrics.loc[idx[target_number.index, metric]],\n",
    "                width=width,\n",
    "                label=model\n",
    "            )\n",
    "            axes[i+1].set_title(metric)\n",
    "            axes[i+1].set_ylabel(metric)\n",
    "            axes[i+1].set_ylim(ylim)\n",
    "\n",
    "\n",
    "        axes[-1].set_xticks(list(range(len(target_number))), target_number.index.to_numpy())\n",
    "        fig.tight_layout()\n",
    "\n",
    "        axes[0].legend()\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {\n",
    "    \"dense\": test_model_eval,\n",
    "    \"lstm\": model_eval\n",
    "}\n",
    "\n",
    "plot_model_metrics_multi(all_metrics, test_ds)\n",
    "fig.savefig(\"figures/combined_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_names:\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(len(models_metrics), sharex=True)\n",
    "\n",
    "# for model, ax in zip(models_metrics, axes.flatten()):\n",
    "#     # Plot the model metrics\n",
    "#     models_metrics[model].plot(kind=\"bar\", ax=ax)\n",
    "#     ax.set_title(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.compile(loss='mae', optimizer='adam')\n",
    "# model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=0)\n",
    "\n",
    "# print(model.evaluate(X_test, y_test))\n",
    "# # 10.704551696777344\n",
    "\n",
    "# # normalize the inputs outside the model\n",
    "# normalizer = Normalization()\n",
    "# normalizer.adapt(X_train)\n",
    "\n",
    "# X_train_normalized = normalizer(X_train)\n",
    "# X_test_normalized = normalizer(X_test)\n",
    "\n",
    "# inputs = Input(shape=[None, 1])\n",
    "# x = LSTM(4, return_sequences=True)(inputs)\n",
    "# x = LSTM(2, return_sequences=True)(x)\n",
    "# x = LSTM(2, return_sequences=True)(x)\n",
    "# x = LSTM(4, return_sequences=True)(x)\n",
    "# x = TimeDistributed((Dense(1)))(x)\n",
    "# model = Model(inputs, x)\n",
    "\n",
    "# model.compile(loss='mae', optimizer='adam')\n",
    "# model.fit(X_train_normalized, y_train, batch_size=32, epochs=10, verbose=0)\n",
    "\n",
    "# print(model.evaluate(X_test_normalized, y_test))\n",
    "# # 10.748750686645508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class TimeSeriesNormalization(layers.Layer):\n",
    "#     def __init__(self, epsilon=1e-6):\n",
    "#         super(TimeSeriesNormalization, self).__init__()\n",
    "#         self.epsilon = epsilon  # To prevent division by zero\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         \"\"\"\n",
    "#         Normalize each time series independently to zero mean and unit variance.\n",
    "\n",
    "#         Args:\n",
    "#             inputs: Tensor of shape (batch_size, time_steps, features)\n",
    "\n",
    "#         Returns:\n",
    "#             Normalized tensor of the same shape\n",
    "#         \"\"\"\n",
    "#         mean = tf.reduce_mean(inputs, axis=1, keepdims=True)  # Compute mean along time axis\n",
    "#         std = tf.math.reduce_std(inputs, axis=1, keepdims=True)  # Compute std along time axis\n",
    "\n",
    "#         return (inputs - mean) / (std + self.epsilon)  # Normalize\n",
    "\n",
    "# # Example usage\n",
    "# batch_size, time_steps, features = 32, 100, 5\n",
    "# input_data = tf.random.normal((batch_size, time_steps, features))  # Simulated time series data\n",
    "\n",
    "# normalization_layer = TimeSeriesNormalization()\n",
    "# normalized_data = normalization_layer(input_data)\n",
    "\n",
    "# print(\"Input shape:\", input_data.shape)\n",
    "# print(\"Normalized shape:\", normalized_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots()\n",
    "# ax.plot(normalized_data.numpy().std(axis=1))\n",
    "# ax.set_ylim(-1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojipml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
