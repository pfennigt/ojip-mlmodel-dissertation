{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas.core.indexing import _IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.core.dtypes.common import is_numeric_dtype, is_object_dtype\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import common_functions as fnc\n",
    "\n",
    "idx: _IndexSlice = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import df, samples, didx, DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plots\n",
    "ax = df.loc[:, didx(\n",
    "    fluorometer=\"MULTI-COLOR-PAM\",\n",
    "    CO2_level=\"Air\", \n",
    "    strain=\"Chlorella vulgaris\",\n",
    "    # SP_color=455\n",
    ")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [Detector V]\")\n",
    "ax.set_title(\"MCPAM - Example\")\n",
    "\n",
    "ax = df.loc[:, didx(fluorometer=\"AquaPen\", CO2_level=\"Air\")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [AU]\")\n",
    "ax.set_title(\"AquaPen - Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the data to be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.loc[\n",
    "    0.01:, : # Exclude data before the light pulse\n",
    "    # didx(\n",
    "    #     fluorometer=\"MULTI-COLOR-PAM\", # Only use MCPAM data\n",
    "    #     strain='Synechocystis sp. PCC 6803', # Only use Synechocystis data\n",
    "    # )\n",
    "].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the map of effects transformed into one-hot encoding\n",
    "effects_map = pd.read_csv(\n",
    "    DATA_PATH / \"effects_map.csv\",\n",
    "    header=[0,1],\n",
    "    index_col=[0,1],\n",
    "    ).astype(float).fillna(0).astype(bool)\n",
    "\n",
    "# Exclude Light intensity and temperature from targets\n",
    "effects_map = effects_map[[\n",
    "    'control_measurement',\n",
    "    'PSII_closed',\n",
    "    'CBB_inhibited',\n",
    "    'TOX_inhibited',\n",
    "    'electron_drain'\n",
    "]]\n",
    "\n",
    "# Get the effects and map the mto the targets\n",
    "effects = samples.loc[dat.columns.get_level_values(0), [\"Effect in PSET\", \"Treatment\"]]\n",
    "\n",
    "targets = effects_map.loc[pd.MultiIndex.from_frame(effects)].droplevel(1, axis=1)\n",
    "targets.index = dat.columns\n",
    "\n",
    "# Make a Multiindex with a duplicated entries\n",
    "targets.columns = pd.MultiIndex.from_arrays([targets.columns, targets.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the conditions as the Multiindex-columns\n",
    "conditions = dat.columns.to_frame()\n",
    "# conditions.index = dat.columns.get_level_values(0)\n",
    "\n",
    "# Select the relevant columns\n",
    "condition_types = pd.Series({\n",
    "    'Strain': \"string\",\n",
    "    'CO2 level': \"numeric\", # There is a meaning to a higher CO2 concentration (maybe make categorical?)\n",
    "    'Cultivation + experiment temperature': \"numeric\",\n",
    "    'Cultivation light intensity': \"numeric\",\n",
    "    'Dark or light acclimated': \"string\",\n",
    "    'Growth light color (nm)': \"string\",\n",
    "    'Fluorometer': \"string\",\n",
    "    'SP color (nm)': \"string\", # There is no linear relationship between wavelength and effect\n",
    "    'SP intensity': \"numeric\",\n",
    "    'OD680 MC-1000': \"numeric\",\n",
    "    'OD720 MC-1000': \"numeric\",\n",
    "})\n",
    "\n",
    "conditions = conditions[condition_types.index]\n",
    "\n",
    "# Replace certain column values\n",
    "\n",
    "# Replace CO2 level with the actual (assumed) numerical ppm\n",
    "conditions[\"CO2 level\"] = conditions[\"CO2 level\"].replace({\n",
    "    \"Air\": \"0.0004\",\n",
    "    \"High CO2\": \"0.05\"\n",
    "}).astype(float)\n",
    "\n",
    "# Replace SP color with categorical value because the numerical gradient is not meaningful\n",
    "conditions[\"SP color (nm)\"] = conditions[\"SP color (nm)\"].astype(str)\n",
    "\n",
    "# Make a Multiindex with a duplicated entries\n",
    "conditions.columns = pd.MultiIndex.from_arrays([conditions.columns, conditions.columns])\n",
    "\n",
    "# Encode conditions in one-hot\n",
    "categorical_conditions = condition_types[condition_types == \"string\"].index.to_numpy()\n",
    "numerical_conditions = condition_types[condition_types != \"string\"].index.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample OJIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of sampled points\n",
    "n_points = 40\n",
    "\n",
    "# Time points, logspaced\n",
    "log_time_points = np.linspace(\n",
    "    np.log10(dat.index[0]),\n",
    "    np.log10(dat.index[-1]),\n",
    "    n_points\n",
    ")\n",
    "time_points = 10 ** log_time_points\n",
    "\n",
    "# Pre-populate the interp function\n",
    "_interp = partial(np.interp, time_points, dat.index)\n",
    "\n",
    "# Interpolate the selected points\n",
    "ojip_sampled = dat.apply(_interp)\n",
    "ojip_sampled.index = pd.MultiIndex.from_product([\n",
    "    [\"ojip\"],\n",
    "    [\"ojip_\" + x for x in log_time_points.round(2).astype(str)]\n",
    "])\n",
    "\n",
    "# Add sampled points to features\n",
    "ojip_sampled = ojip_sampled.T\n",
    "\n",
    "# Subset the data to the samples and time to be included in the analysis \n",
    "ax = dat.plot(legend=False)\n",
    "\n",
    "for t in time_points:\n",
    "    ax.axvline(t)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Add data types\n",
    "ojip_types = pd.Series({\"ojip\":\"time-series-gradients\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all data sets\n",
    "\n",
    "dat_sets = [\n",
    "    conditions,\n",
    "    ojip_sampled,\n",
    "    targets\n",
    "]\n",
    "\n",
    "dat_full = pd.concat(dat_sets, axis=1)\n",
    "\n",
    "# Make into dataset\n",
    "dat_ds = fnc.df_to_dataset(dat_full, targets=list(targets.columns.levels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set a random seed for UMAP\n",
    "# UMAP_seed = 2025\n",
    "\n",
    "# # Scale the features\n",
    "# df_features_scaled = StandardScaler().fit_transform(df_features.values)\n",
    "\n",
    "# # Create the UMAP embedding\n",
    "# reducer = umap.UMAP(random_state=UMAP_seed)\n",
    "# embedding = pd.DataFrame(\n",
    "#     reducer.fit_transform(df_features_scaled),\n",
    "#     index=df_features.index,\n",
    "#     columns=[\"UMAP_1\", \"UMAP_2\"]\n",
    "# ).reset_index()\n",
    "\n",
    "# # Plot\n",
    "# categories = df.columns.names[1:]\n",
    "# fig, axes = plt.subplots(\n",
    "#     int(np.ceil(len(categories)/3)),\n",
    "#     3,\n",
    "#     figsize=(7,15),\n",
    "#     sharey=True,\n",
    "#     sharex=True,\n",
    "# )\n",
    "\n",
    "# for category, ax in zip(categories, axes.flatten()):\n",
    "#     sns.scatterplot(\n",
    "#         embedding,\n",
    "#         x=\"UMAP_1\",\n",
    "#         y=\"UMAP_2\",\n",
    "#         hue=category,\n",
    "#         ax=ax,\n",
    "#         legend=False\n",
    "#     )\n",
    "#     ax.set_title(category)\n",
    "\n",
    "#     if len(embedding[category].value_counts()) == 1:\n",
    "#         ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot targest on UMAP\n",
    "# # Add UMAP to targets\n",
    "# embedding_targets = pd.concat([\n",
    "#     targets.droplevel(-1, axis=1).droplevel(list(range(1,20)), axis=0),\n",
    "#     embedding.set_index(\"Label\").loc[:, [\"UMAP_1\", \"UMAP_2\"]],\n",
    "# ], axis=1)\n",
    "\n",
    "# # Plot\n",
    "# categories = effects_map.columns.get_level_values(0)\n",
    "# fig, axes = plt.subplots(\n",
    "#     int(np.ceil(len(categories)/3)),\n",
    "#     3,\n",
    "#     figsize=(7,7),\n",
    "#     sharex=True,\n",
    "#     sharey=True\n",
    "# )\n",
    "\n",
    "# for category, ax in zip(categories, axes.flatten()):\n",
    "#     sns.scatterplot(\n",
    "#         embedding_targets,\n",
    "#         x=\"UMAP_1\",\n",
    "#         y=\"UMAP_2\",\n",
    "#         hue=category,\n",
    "#         ax=ax,\n",
    "#         legend=False\n",
    "#     )\n",
    "#     ax.set_title(category)\n",
    "\n",
    "#     if len(embedding_targets[category].value_counts()) == 1:\n",
    "#         ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the types of all features\n",
    "feature_types = pd.concat([\n",
    "    condition_types,\n",
    "    ojip_types\n",
    "])\n",
    "\n",
    "train_ds = dat_ds\n",
    "\n",
    "# Create containers for inputs and encodings\n",
    "all_inputs = {}\n",
    "encoded_features_dict = {}\n",
    "encoded_features = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name, col_dtype in feature_types.items():\n",
    "\n",
    "    print(col_name)\n",
    "    # Create a numeric normalisation layer\n",
    "    if col_dtype == \"numeric\":\n",
    "        col = layers.Input(shape=(1,), name=col_name)\n",
    "        normalization_layer = fnc.get_normalization_layer(col_name, train_ds)\n",
    "        encoded_col = normalization_layer(col)\n",
    "    \n",
    "    # Create a string enconding layer, could also work for integer encoding\n",
    "    elif col_dtype == \"string\":\n",
    "        col = layers.Input(shape=(1,), name=col_name, dtype='string')\n",
    "        encoding_layer = fnc.get_category_encoding_layer(name=col_name,\n",
    "                                                    dataset=train_ds,\n",
    "                                                    dtype='string',\n",
    "                                                    max_tokens=5)\n",
    "        encoded_col = encoding_layer(col)\n",
    "\n",
    "    # Create a layer to normalise time series and calculate gradients\n",
    "    elif col_dtype == \"time-series-gradients\":\n",
    "        col = layers.Input(shape=(dat_full[col_name].shape[1],), name=col_name)\n",
    "        reshaped_col = layers.Reshape((dat_full[col_name].shape[1], 1))(col)\n",
    "        normalization_layer = fnc.NormalizedTimeSeriesWithDerivatives()\n",
    "        encoded_col = normalization_layer(reshaped_col)\n",
    "        encoded_col = layers.Flatten()(encoded_col)\n",
    "        \n",
    "    else:\n",
    "        raise KeyError(f\"No handling for col_dtype {col_dtype} defined\")\n",
    "\n",
    "    all_inputs[col_name] = col\n",
    "    encoded_features.append(encoded_col)\n",
    "    encoded_features_dict[col_name] = encoded_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "all_features = layers.concatenate(encoded_features)\n",
    "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(targets.shape[1])(x)\n",
    "\n",
    "model = keras.Model(all_inputs, encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile test model\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `rankdir='LR'` to make the graph horizontal.\n",
    "keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dat_full.iloc[0,:]\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([sample[name].to_numpy()]) for name in sample.index.levels[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_working = []\n",
    "outputs = {}\n",
    "for i in range(len(input_dict)):\n",
    "    try:\n",
    "        test_feature = list(input_dict.keys())[i]\n",
    "\n",
    "        test_model = keras.Model({test_feature:all_inputs[test_feature]}, encoded_features_dict[test_feature])\n",
    "        test_model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "        outputs[test_feature] = test_model.predict({test_feature:input_dict[test_feature]})\n",
    "    except Exception:\n",
    "        not_working.append(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "#     df_features.astype(int),\n",
    "#     targets.astype(int), \n",
    "#     test_size=0.2, \n",
    "#     random_state=42,\n",
    "#     stratify=targets.values\n",
    "# )\n",
    "\n",
    "# if not np.all(Y_train.drop_duplicates().sum(axis=0) == 1):\n",
    "#     raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# # Scale data\n",
    "# scaler_X = StandardScaler().fit(X_train.values)\n",
    "# scaler_Y = StandardScaler().fit(Y_train.values)\n",
    "\n",
    "# X_train_scaled = scaler_X.transform(X_train.values)\n",
    "# X_test_scaled = scaler_X.transform(X_test.values)\n",
    "\n",
    "# Y_train_scaled = scaler_Y.transform(Y_train.values)\n",
    "# Y_test_scaled = scaler_Y.transform(Y_test.values)\n",
    "\n",
    "# print(f\"Training model to recognize {Y_train.shape[1]} target features.\\nUsing {X_train.shape[0]} samples with {X_train.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "# Define the feature inputs\n",
    "ojip_input = Input(shape=(X_train_scaled.shape[1],), name=\"ojip_input\")\n",
    "\n",
    "x = keras.layers.Reshape((X_train_scaled.shape[1],1), name=\"LSTM_1_reshape\")(ojip_input)\n",
    "\n",
    "# Hidden layer\n",
    "x = LSTM(64, activation=\"tanh\", name=\"LSTM_1\")(x)\n",
    "x = Dropout(0.3, name=\"LSTM_1_Dropout\")(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(Y_train_scaled.shape[1], activation=\"relu\", name=\"prediction\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[ojip_input],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "\n",
    "##  Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError]\n",
    ")\n",
    "\n",
    "##  Train the model\n",
    "history = model.fit(\n",
    "    [X_train_scaled],\n",
    "    [Y_train_scaled],\n",
    "    validation_split=0.1,\n",
    "    epochs=500,\n",
    "    verbose=0,\n",
    "    batch_size=10,\n",
    "    callbacks=[TqdmCallback(verbose=1)]\n",
    ")\n",
    "\n",
    "plot_loss_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(models_metrics), sharex=True)\n",
    "\n",
    "for model, ax in zip(models_metrics, axes.flatten()):\n",
    "    # Plot the model metrics\n",
    "    models_metrics[model].plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=0)\n",
    "\n",
    "print(model.evaluate(X_test, y_test))\n",
    "# 10.704551696777344\n",
    "\n",
    "# normalize the inputs outside the model\n",
    "normalizer = Normalization()\n",
    "normalizer.adapt(X_train)\n",
    "\n",
    "X_train_normalized = normalizer(X_train)\n",
    "X_test_normalized = normalizer(X_test)\n",
    "\n",
    "inputs = Input(shape=[None, 1])\n",
    "x = LSTM(4, return_sequences=True)(inputs)\n",
    "x = LSTM(2, return_sequences=True)(x)\n",
    "x = LSTM(2, return_sequences=True)(x)\n",
    "x = LSTM(4, return_sequences=True)(x)\n",
    "x = TimeDistributed((Dense(1)))(x)\n",
    "model = Model(inputs, x)\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.fit(X_train_normalized, y_train, batch_size=32, epochs=10, verbose=0)\n",
    "\n",
    "print(model.evaluate(X_test_normalized, y_test))\n",
    "# 10.748750686645508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class TimeSeriesNormalization(layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        super(TimeSeriesNormalization, self).__init__()\n",
    "        self.epsilon = epsilon  # To prevent division by zero\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Normalize each time series independently to zero mean and unit variance.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, time_steps, features)\n",
    "\n",
    "        Returns:\n",
    "            Normalized tensor of the same shape\n",
    "        \"\"\"\n",
    "        mean = tf.reduce_mean(inputs, axis=1, keepdims=True)  # Compute mean along time axis\n",
    "        std = tf.math.reduce_std(inputs, axis=1, keepdims=True)  # Compute std along time axis\n",
    "\n",
    "        return (inputs - mean) / (std + self.epsilon)  # Normalize\n",
    "\n",
    "# Example usage\n",
    "batch_size, time_steps, features = 32, 100, 5\n",
    "input_data = tf.random.normal((batch_size, time_steps, features))  # Simulated time series data\n",
    "\n",
    "normalization_layer = TimeSeriesNormalization()\n",
    "normalized_data = normalization_layer(input_data)\n",
    "\n",
    "print(\"Input shape:\", input_data.shape)\n",
    "print(\"Normalized shape:\", normalized_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(normalized_data.numpy().std(axis=1))\n",
    "ax.set_ylim(-1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojipml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
