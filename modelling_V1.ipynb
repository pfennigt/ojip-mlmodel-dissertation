{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas.core.indexing import _IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.core.dtypes.common import is_numeric_dtype, is_object_dtype\n",
    "import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import common_functions as fnc\n",
    "\n",
    "idx: _IndexSlice = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import df, samples, didx, DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plots\n",
    "ax = df.loc[:, didx(\n",
    "    fluorometer=\"MULTI-COLOR-PAM\",\n",
    "    CO2_level=\"Air\", \n",
    "    strain=\"Chlorella vulgaris\",\n",
    "    # SP_color=455\n",
    ")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [Detector V]\")\n",
    "ax.set_title(\"MCPAM - Example\")\n",
    "\n",
    "ax = df.loc[:, didx(fluorometer=\"AquaPen\", CO2_level=\"Air\")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [AU]\")\n",
    "ax.set_title(\"AquaPen - Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the data to be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.loc[\n",
    "    0.01:, : # Exclude data before the light pulse\n",
    "    # didx(\n",
    "    #     fluorometer=\"MULTI-COLOR-PAM\", # Only use MCPAM data\n",
    "    #     strain='Synechocystis sp. PCC 6803', # Only use Synechocystis data\n",
    "    # )\n",
    "].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = pd.read_csv(\n",
    "    DATA_PATH / \"effects_map.csv\",\n",
    "    header=[0,1],\n",
    "    index_col=[0,1],\n",
    "    ).astype(float).fillna(0).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the map of effects transformed into one-hot encoding\n",
    "effects_map_raw = pd.read_csv(\n",
    "    DATA_PATH / \"effects_map.csv\",\n",
    "    header=[0,1],\n",
    "    index_col=[0,1],\n",
    "    ).astype(float).fillna(0).astype(bool)\n",
    "\n",
    "# Exclude Light intensity and temperature from targets\n",
    "effects_map = effects_map_raw[[\n",
    "    'control_measurement',\n",
    "    'PSII_closed',\n",
    "    'CBB_inhibited',\n",
    "    'TOX_inhibited',\n",
    "    'electron_drain'\n",
    "]]\n",
    "\n",
    "# Set High light as a closing factor of PSII\n",
    "effects_map.loc[:,'PSII_closed'] = np.logical_or(\n",
    "    effects_map['PSII_closed'].to_numpy(),\n",
    "    effects_map_raw[\"high_light\"].to_numpy()\n",
    ")\n",
    "\n",
    "# Ass high and low temperature as \n",
    "effects_map.loc[:,'control_measurement'] = np.logical_or(\n",
    "    effects_map['control_measurement'].to_numpy(),\n",
    "    effects_map_raw[\"low_temperature\"].to_numpy()\n",
    ")\n",
    "effects_map.loc[:,'control_measurement'] = np.logical_or(\n",
    "    effects_map['control_measurement'].to_numpy(),\n",
    "    effects_map_raw[\"high_temperature\"].to_numpy()\n",
    ")\n",
    "\n",
    "# Get the effects and map the mto the targets\n",
    "effects = samples.loc[dat.columns.get_level_values(0), [\"Effect in PSET\", \"Treatment\"]]\n",
    "\n",
    "targets = effects_map.loc[pd.MultiIndex.from_frame(effects)].droplevel(1, axis=1)\n",
    "targets = targets.astype(int)\n",
    "targets.index = dat.columns\n",
    "\n",
    "# Make a Multiindex with a duplicated entries\n",
    "target_names = targets.columns\n",
    "targets.columns = pd.MultiIndex.from_arrays([targets.columns, targets.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Message from Tomas:**\n",
    "\n",
    "INPUTS\n",
    "- Strain: Synechocystis / Chlorella. Later, when we have more strains measured, we can make it more general like green algae/cyanobacteria.\n",
    "- Temperature: **it could be enough to keep it in intervals, like e.g. 15-20 / 20-25 / 25-30 / 30-35 etc.**\n",
    "- Light acclimation state: light-acclimated / dark-acclimated\n",
    "- OD: needs to be standardized = measured by one device since each device measure OD differently.  In the dataset we now have it for Multi-Cultivator, but it would be better to refer to UV-Vis and 1 cm cuvette. Alternatively, chlorophyll content could work well. I can provide all these values, based on additional measurements that we have already performed.\n",
    "- SP color: **blue / orange-red could be enough**\n",
    "- SP Intensity: saturating / non-saturating. This parameter definitely matters, but it could be tricky for the users to validate if SP is saturating or not. So, alternatively, the user can provide SP intensity (**perhaps again in some intervals like < 1000 / 1000-1500 / 1500-2000 / 2000-2500 / >2500**) and we can evaluate by our algorithm if the SP was saturating, based on the selection of strain, OD, SP color and SP intensity.\n",
    "- Fluorometer: MCPAM / AquaPen - I would keep it for now, since I am still not sure if there is any fundamental difference between MC-PAM, AquaPen and FL-6000, or not. But in general, it should be only SP color and SP intensity that matter the most\n",
    "- These parameters might be not necessary, but I am not sure - the OJIP curves were different under these specific acclimations, so it could make the predictions better if we discriminate also these acclimations - but on the other hand, it might be overkill:\n",
    "  - CO2 level\n",
    "  - Growth light color\n",
    "\n",
    "OUTPUTS\n",
    "- PSII closed: DCMU, high light\n",
    "- CBB inhibited: Glycolaldehyde\n",
    "- TOX inhibited: KCN\n",
    "- Electron drain from PSI: Methylviologen. Btw. yes, this effect should be the opposite of Glycolaldehyde treatment\n",
    "- **PQ redox state: reduced / oxidized**\n",
    "- It would be great to include more inhibitors, to describe more effects -we can do that later:\n",
    "  - Cyt b6/f closed: DBMIB\n",
    "  - Cyclic electron flow around PSI closed: Antimycin A3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the conditions as the Multiindex-columns\n",
    "conditions = dat.columns.to_frame()\n",
    "# conditions.index = dat.columns.get_level_values(0)\n",
    "\n",
    "# Select the relevant columns\n",
    "condition_types = pd.Series({\n",
    "    'Strain': \"string\",\n",
    "    'CO2 level': \"numeric\", # There is a meaning to a higher CO2 concentration (maybe make categorical?)\n",
    "    'Cultivation + experiment temperature': \"numeric\",\n",
    "    'Cultivation light intensity': \"numeric\",\n",
    "    'Dark or light acclimated': \"string\",\n",
    "    'Growth light color (nm)': \"string\",\n",
    "    'Fluorometer': \"string\",\n",
    "    'SP color (nm)': \"string\", # There is no linear relationship between wavelength and effect\n",
    "    'SP intensity': \"numeric\",\n",
    "    'OD680 MC-1000': \"numeric\",\n",
    "    'OD720 MC-1000': \"numeric\",\n",
    "})\n",
    "\n",
    "conditions = conditions[condition_types.index]\n",
    "\n",
    "# Replace certain column values\n",
    "\n",
    "# Replace CO2 level with the actual (assumed) numerical ppm\n",
    "conditions[\"CO2 level\"] = conditions[\"CO2 level\"].replace({\n",
    "    \"Air\": \"0.0004\",\n",
    "    \"High CO2\": \"0.05\"\n",
    "}).astype(float)\n",
    "\n",
    "# Replace SP color with categorical value because the numerical gradient is not meaningful\n",
    "conditions[\"SP color (nm)\"] = conditions[\"SP color (nm)\"].astype(str)\n",
    "\n",
    "# Make a Multiindex with a duplicated entries\n",
    "conditions.columns = pd.MultiIndex.from_arrays([conditions.columns, conditions.columns])\n",
    "\n",
    "# Encode conditions in one-hot\n",
    "categorical_conditions = condition_types[condition_types == \"string\"].index.to_numpy()\n",
    "numerical_conditions = condition_types[condition_types != \"string\"].index.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample OJIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of sampled points\n",
    "n_points = 40\n",
    "\n",
    "# Time points, logspaced\n",
    "log_time_points = np.linspace(\n",
    "    np.log10(dat.index[0]),\n",
    "    np.log10(dat.index[-1]),\n",
    "    n_points\n",
    ")\n",
    "time_points = 10 ** log_time_points\n",
    "\n",
    "# Pre-populate the interp function\n",
    "_interp = partial(np.interp, time_points, dat.index)\n",
    "\n",
    "# Interpolate the selected points\n",
    "ojip_sampled = dat.apply(_interp)\n",
    "ojip_sampled.index = pd.MultiIndex.from_product([\n",
    "    [\"ojip\"],\n",
    "    [\"ojip_\" + x for x in log_time_points.round(2).astype(str)]\n",
    "])\n",
    "\n",
    "# Add sampled points to features\n",
    "ojip_sampled = ojip_sampled.T\n",
    "\n",
    "# Subset the data to the samples and time to be included in the analysis \n",
    "ax = dat.plot(legend=False)\n",
    "\n",
    "for t in time_points:\n",
    "    ax.axvline(t, c=\"k\", alpha=0.5)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Add data types\n",
    "ojip_types = pd.Series({\"ojip\":\"time-series-gradients\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all data sets\n",
    "dat_sets = [\n",
    "    conditions,\n",
    "    ojip_sampled,\n",
    "    targets\n",
    "]\n",
    "dat_full = pd.concat(dat_sets, axis=1)\n",
    "\n",
    "# Split data into training set\n",
    "dat_train, _dat_trainval = train_test_split(\n",
    "    dat_full,\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=dat_full[target_names]\n",
    ")\n",
    "\n",
    "# Split data into test and validation set\n",
    "dat_test, dat_val = train_test_split(\n",
    "    _dat_trainval,\n",
    "    test_size=0.5, \n",
    "    random_state=42,\n",
    "    stratify=_dat_trainval[target_names]\n",
    ")\n",
    "\n",
    "print(f\"Dimensions train: {dat_train.shape}, test: {dat_test.shape}, val: {dat_val.shape}\")\n",
    "\n",
    "\n",
    "if not np.all(dat_train[target_names].drop_duplicates().sum(axis=0) == 1):\n",
    "    raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# Make into dataset\n",
    "train_ds = fnc.df_to_dataset(dat_train, targets=target_names, batch_size=64, shuffle=True)\n",
    "test_ds = fnc.df_to_dataset(dat_test, targets=target_names, shuffle=False)\n",
    "val_ds = fnc.df_to_dataset(dat_val, targets=target_names, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the types of all features\n",
    "feature_types = pd.concat([\n",
    "    condition_types,\n",
    "    ojip_types\n",
    "])\n",
    "\n",
    "# Create containers for inputs and encodings\n",
    "all_inputs = {}\n",
    "encoded_features_dict = {}\n",
    "encoded_features = []\n",
    "\n",
    "# Encode all features\n",
    "for col_name, col_dtype in feature_types.items():\n",
    "    # Create a numeric normalisation layer\n",
    "    if col_dtype == \"numeric\":\n",
    "        col = layers.Input(shape=(1,), name=col_name)\n",
    "        normalization_layer = fnc.get_normalization_layer(col_name, train_ds)\n",
    "        encoded_col = normalization_layer(col)\n",
    "    \n",
    "    # Create a string enconding layer, could also work for integer encoding\n",
    "    elif col_dtype == \"string\":\n",
    "        col = layers.Input(shape=(1,), name=col_name, dtype='string')\n",
    "        encoding_layer = fnc.get_category_encoding_layer(name=col_name,\n",
    "                                                    dataset=train_ds,\n",
    "                                                    dtype='string',\n",
    "                                                    max_tokens=5)\n",
    "        encoded_col = encoding_layer(col)\n",
    "\n",
    "    # Create a layer to normalise time series and calculate gradients\n",
    "    elif col_dtype == \"time-series-gradients\":\n",
    "        col = layers.Input(shape=(dat_full[col_name].shape[1],), name=col_name)\n",
    "        reshaped_col = layers.Reshape((dat_full[col_name].shape[1], 1))(col)\n",
    "        normalization_layer = fnc.NormalizedTimeSeriesWithDerivatives()\n",
    "        encoded_col = normalization_layer(reshaped_col)\n",
    "        \n",
    "    else:\n",
    "        raise KeyError(f\"No handling for col_dtype {col_dtype} defined\")\n",
    "\n",
    "    all_inputs[col_name] = col\n",
    "    encoded_features.append(encoded_col)\n",
    "    encoded_features_dict[col_name] = encoded_col\n",
    "\n",
    "# Define the preprocessing layer as a model\n",
    "preprocessing_layer = keras.Model(\n",
    "    all_inputs,\n",
    "    encoded_features_dict,\n",
    "    name=\"preprocessing_layer\"\n",
    ")\n",
    "\n",
    "# Plot the preprocessing layer\n",
    "keras.utils.plot_model(\n",
    "    preprocessing_layer,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    # rankdir=\"LR\",\n",
    "    to_file=\"figures/test_preprocessing.png\",\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create UMAP mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the outputs of the preprocessing layer to perform UMAP\n",
    "preprocessed = preprocessing_layer(all_inputs)\n",
    "\n",
    "# Append the derivatives of the ojip signal to the end to create a long feature vector\n",
    "preprocessed[\"ojip\"] = layers.Flatten()(preprocessed[\"ojip\"])\n",
    "\n",
    "concatenated_preprocess = keras.Model(\n",
    "    all_inputs,\n",
    "    layers.concatenate(list(preprocessed.values())),\n",
    "    name=\"concatenation_layer\"\n",
    ")\n",
    "\n",
    "# Set a random seed for UMAP\n",
    "UMAP_seed = 2025\n",
    "\n",
    "# Scale the features\n",
    "df_features_scaled = concatenated_preprocess.predict(\n",
    "    fnc.get_dataset_from_input_df(dat_full, all_inputs)\n",
    ")\n",
    "\n",
    "# Create the UMAP embedding\n",
    "reducer = umap.UMAP(random_state=UMAP_seed)\n",
    "embedding = pd.DataFrame(\n",
    "    reducer.fit_transform(df_features_scaled),\n",
    "    index=dat_full.index,\n",
    "    columns=[\"UMAP_1\", \"UMAP_2\"]\n",
    ").reset_index()\n",
    "\n",
    "# Plot\n",
    "categories = df.columns.names[1:]\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,15),\n",
    "    sharey=True,\n",
    "    sharex=True,\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/umap_conditions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot targest on UMAP\n",
    "# Add UMAP to targets\n",
    "embedding_targets = pd.concat([\n",
    "    targets.droplevel(-1, axis=1).droplevel(list(range(1,20)), axis=0),\n",
    "    embedding.set_index(\"Label\").loc[:, [\"UMAP_1\", \"UMAP_2\"]],\n",
    "], axis=1)\n",
    "\n",
    "# Plot\n",
    "categories = effects_map.columns.get_level_values(0)\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,5),\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding_targets,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding_targets[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/umap_conditions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_output_to_dict(dense_output):\n",
    "    outputs = {\n",
    "        \"control_measurement\": layers.Lambda(lambda x: tf.expand_dims(x[:, 0], axis=-1), name=\"control_measurement\")(dense_output),\n",
    "        \"PSII_closed\": layers.Lambda(lambda x: tf.expand_dims(x[:, 1], axis=-1), name=\"PSII_closed\")(dense_output),\n",
    "        \"CBB_inhibited\": layers.Lambda(lambda x: tf.expand_dims(x[:, 2], axis=-1), name=\"CBB_inhibited\")(dense_output),\n",
    "        \"TOX_inhibited\": layers.Lambda(lambda x: tf.expand_dims(x[:, 3], axis=-1), name=\"TOX_inhibited\")(dense_output),\n",
    "        \"electron_drain\": layers.Lambda(lambda x: tf.expand_dims(x[:, 4], axis=-1), name=\"electron_drain\")(dense_output),\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model\n",
    "# Get the preprocessed inputs\n",
    "preprocessed = preprocessing_layer(all_inputs)\n",
    "\n",
    "# Append the derivatives of the ojip signal to the end to create a long feature vector\n",
    "preprocessed[\"ojip\"] = layers.Flatten()(preprocessed[\"ojip\"])\n",
    "\n",
    "# Concatenate all features\n",
    "x = layers.concatenate(list(preprocessed.values()))\n",
    "\n",
    "# Dense hidden layer\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Dense layer for output calculation\n",
    "# Uses sigmoid activation function for output between 0 and 1\n",
    "dense_output = layers.Dense(targets.shape[1], activation=\"sigmoid\")(x)\n",
    "\n",
    "# Split the output into a dictionary\n",
    "outputs = split_output_to_dict(dense_output)\n",
    "\n",
    "\n",
    "# Create and compile test model\n",
    "test_model = keras.Model(all_inputs, outputs)\n",
    "test_model.compile(\n",
    "    loss='binary_crossentropy', # Expects output to be multiple probabilities for classes\n",
    "    optimizer='adam',\n",
    "    metrics={key:[ # Calculate metrics for each target\n",
    "        keras.metrics.BinaryAccuracy(threshold=0.5, name=\"..binary_accuracy\"), # Accuracy (assumes threshold 0.5)\n",
    "        keras.metrics.Recall(thresholds=0.5, name=\"..recall\"), # Recall (assumes threshold 0.5)\n",
    "        keras.metrics.Precision(thresholds=0.5, name=\"..precision\"), # Precision (assumes threshold 0.5)\n",
    "        keras.metrics.F1Score(threshold=0.5, name=\"..f1_score\"), # F1-score (assumes threshold 0.5)\n",
    "        ] for key in target_names},\n",
    ")\n",
    "\n",
    "# Plot the model\n",
    "keras.utils.plot_model(\n",
    "    test_model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    to_file=\"figures/test_model.png\",\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Use tensorboard to track the training\n",
    "use_tensorboard = False\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = [keras.callbacks.TensorBoard(log_dir=log_dir)] if use_tensorboard else []\n",
    "\n",
    "test_model_history = test_model.fit(\n",
    "    train_ds,\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[\n",
    "        TqdmCallback(verbose=1)\n",
    "    ] + tensorboard_callback\n",
    ")\n",
    "\n",
    "# Plot the loss over the Epochs\n",
    "fnc.plot_loss_development(test_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_model_eval = test_model.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "fnc.plot_model_metrics(test_model_eval, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplary prediction\n",
    "fnc.predict_model_from_df(test_model, dat_test).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "# Get the preprocessed inputs\n",
    "seperated_inputs = preprocessing_layer(all_inputs)\n",
    "\n",
    "# Add an LSTM layer to the OJIP input\n",
    "seperated_inputs[\"ojip\"] = layers.LSTM(16, activation=\"tanh\", name=\"LSTM_ojip\")(seperated_inputs[\"ojip\"])\n",
    "seperated_inputs[\"ojip\"] = layers.Dropout(0.5, name=\"LSTM_1_Dropout\")(seperated_inputs[\"ojip\"])\n",
    "\n",
    "# Concatenate all features\n",
    "x = layers.concatenate(list(seperated_inputs.values()))\n",
    "\n",
    "# Dense hidden layer\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Dense layer for output calculation\n",
    "# Uses sigmoid activation function for output between 0 and 1\n",
    "dense_output = layers.Dense(targets.shape[1], activation=\"sigmoid\")(x)\n",
    "\n",
    "# Split the output into a dictionary\n",
    "outputs = split_output_to_dict(dense_output)\n",
    "\n",
    "# Create and compile test model\n",
    "model = keras.Model(all_inputs, outputs)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', # Expects output to be multiple probabilities for classes\n",
    "    optimizer='adam',\n",
    "    metrics={key:[ # Calculate metrics for each target\n",
    "        keras.metrics.BinaryAccuracy(threshold=0.5, name=\"..binary_accuracy\"), # Accuracy (assumes threshold 0.5)\n",
    "        keras.metrics.Recall(thresholds=0.5, name=\"..recall\"), # Recall (assumes threshold 0.5)\n",
    "        keras.metrics.Precision(thresholds=0.5, name=\"..precision\"), # Precision (assumes threshold 0.5)\n",
    "        keras.metrics.F1Score(threshold=0.5, name=\"..f1_score\"), # F1-score (assumes threshold 0.5)\n",
    "        ] for key in target_names},\n",
    ")\n",
    "\n",
    "# Plot the model\n",
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    to_file=\"figures/model.png\",\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Use tensorboard to track the training\n",
    "use_tensorboard = False\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = [keras.callbacks.TensorBoard(log_dir=log_dir)] if use_tensorboard else []\n",
    "\n",
    "model_history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=800,\n",
    "    verbose=0,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[\n",
    "        TqdmCallback(verbose=1)\n",
    "    ] + tensorboard_callback\n",
    ")\n",
    "\n",
    "# Plot the loss over the Epochs\n",
    "fnc.plot_loss_development(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(train_ds.map(lambda x,y: y).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final metrics of a model\n",
    "def plot_history_metrics(history, fit_dataset, val_dataset):\n",
    "    # Get the targets from the dataset to count occurrences\n",
    "    target_numbers={}\n",
    "    for nam, ds in {\"fit\":fit_dataset, \"val\":val_dataset}.items():\n",
    "        _targets_subsets = list(ds.map(lambda x,y: y).as_numpy_iterator())\n",
    "        _targets = [pd.DataFrame({k:v.flatten() for k,v in _targets_subset.items()}) for _targets_subset in _targets_subsets]\n",
    "        _targets = pd.concat(_targets, axis=0)\n",
    "\n",
    "        target_numbers[nam] = _targets.sum().sort_values(ascending=False)\n",
    "\n",
    "    metrics = history.history\n",
    "\n",
    "    # Get all metrics and assign them to the targets\n",
    "    plot_metrics = pd.DataFrame(\n",
    "        {tuple(k.split(\"_..\")): v for k,v in metrics.items() if not k.endswith(\"loss\")}\n",
    "    )\n",
    "\n",
    "    print(plot_metrics.columns.levels[0])\n",
    "\n",
    "    metrics_names = plot_metrics.columns.levels[1]\n",
    "    targets_names = _targets.columns\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axes = plt.subplots(\n",
    "        len(targets_names), \n",
    "        2,\n",
    "        figsize=(7,7),\n",
    "        sharex=True\n",
    "    )\n",
    "\n",
    "    # Plot each target\n",
    "    for j, val in enumerate([\"fit\", \"val\"]):\n",
    "        for i, _target in enumerate(targets_names):\n",
    "            prefix = \"val_\" if val==\"val\" else \"\"\n",
    "            target = prefix + _target\n",
    "            axes[i, j].plot(\n",
    "                plot_metrics.loc[:, idx[target, metrics_names]],\n",
    "                label=metrics_names\n",
    "            )\n",
    "            axes[i, j].set_title(f\"{target} (n={target_numbers[val][_target]})\")\n",
    "            axes[i, j].set_ylabel(target)\n",
    "\n",
    "        axes[-1, j].set_xlabel(\"Epoch No.\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    axes[0, -1].legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_metrics(history, train_ds, val_dataset=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "model_eval = model.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "fnc.plot_model_metrics(model_eval, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(models_metrics), sharex=True)\n",
    "\n",
    "for model, ax in zip(models_metrics, axes.flatten()):\n",
    "    # Plot the model metrics\n",
    "    models_metrics[model].plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.compile(loss='mae', optimizer='adam')\n",
    "# model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=0)\n",
    "\n",
    "# print(model.evaluate(X_test, y_test))\n",
    "# # 10.704551696777344\n",
    "\n",
    "# # normalize the inputs outside the model\n",
    "# normalizer = Normalization()\n",
    "# normalizer.adapt(X_train)\n",
    "\n",
    "# X_train_normalized = normalizer(X_train)\n",
    "# X_test_normalized = normalizer(X_test)\n",
    "\n",
    "# inputs = Input(shape=[None, 1])\n",
    "# x = LSTM(4, return_sequences=True)(inputs)\n",
    "# x = LSTM(2, return_sequences=True)(x)\n",
    "# x = LSTM(2, return_sequences=True)(x)\n",
    "# x = LSTM(4, return_sequences=True)(x)\n",
    "# x = TimeDistributed((Dense(1)))(x)\n",
    "# model = Model(inputs, x)\n",
    "\n",
    "# model.compile(loss='mae', optimizer='adam')\n",
    "# model.fit(X_train_normalized, y_train, batch_size=32, epochs=10, verbose=0)\n",
    "\n",
    "# print(model.evaluate(X_test_normalized, y_test))\n",
    "# # 10.748750686645508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class TimeSeriesNormalization(layers.Layer):\n",
    "#     def __init__(self, epsilon=1e-6):\n",
    "#         super(TimeSeriesNormalization, self).__init__()\n",
    "#         self.epsilon = epsilon  # To prevent division by zero\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         \"\"\"\n",
    "#         Normalize each time series independently to zero mean and unit variance.\n",
    "\n",
    "#         Args:\n",
    "#             inputs: Tensor of shape (batch_size, time_steps, features)\n",
    "\n",
    "#         Returns:\n",
    "#             Normalized tensor of the same shape\n",
    "#         \"\"\"\n",
    "#         mean = tf.reduce_mean(inputs, axis=1, keepdims=True)  # Compute mean along time axis\n",
    "#         std = tf.math.reduce_std(inputs, axis=1, keepdims=True)  # Compute std along time axis\n",
    "\n",
    "#         return (inputs - mean) / (std + self.epsilon)  # Normalize\n",
    "\n",
    "# # Example usage\n",
    "# batch_size, time_steps, features = 32, 100, 5\n",
    "# input_data = tf.random.normal((batch_size, time_steps, features))  # Simulated time series data\n",
    "\n",
    "# normalization_layer = TimeSeriesNormalization()\n",
    "# normalized_data = normalization_layer(input_data)\n",
    "\n",
    "# print(\"Input shape:\", input_data.shape)\n",
    "# print(\"Normalized shape:\", normalized_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots()\n",
    "# ax.plot(normalized_data.numpy().std(axis=1))\n",
    "# ax.set_ylim(-1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojipml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
