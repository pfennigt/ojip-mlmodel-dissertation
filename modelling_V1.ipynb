{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas.core.indexing import _IndexSlice\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.core.dtypes.common import is_numeric_dtype, is_object_dtype\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import common_functions as fnc\n",
    "\n",
    "idx: _IndexSlice = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import df, samples, didx, DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plots\n",
    "ax = df.loc[:, didx(\n",
    "    fluorometer=\"MULTI-COLOR-PAM\",\n",
    "    CO2_level=\"Air\", \n",
    "    strain=\"Chlorella vulgaris\",\n",
    "    # SP_color=455\n",
    ")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [Detector V]\")\n",
    "ax.set_title(\"MCPAM - Example\")\n",
    "\n",
    "ax = df.loc[:, didx(fluorometer=\"AquaPen\", CO2_level=\"Air\")].dropna().plot(legend=False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Time [ms]\")\n",
    "ax.set_ylabel(\"Fluorescence [AU]\")\n",
    "ax.set_title(\"AquaPen - Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the data to be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.loc[\n",
    "    0.01:, : # Exclude data before the light pulse\n",
    "    # didx(\n",
    "    #     fluorometer=\"MULTI-COLOR-PAM\", # Only use MCPAM data\n",
    "    #     strain='Synechocystis sp. PCC 6803', # Only use Synechocystis data\n",
    "    # )\n",
    "].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the map of effects transformed into one-hot encoding\n",
    "effects_map = pd.read_csv(\n",
    "    DATA_PATH / \"effects_map.csv\",\n",
    "    header=[0,1],\n",
    "    index_col=[0,1],\n",
    "    ).astype(float).fillna(0).astype(bool)\n",
    "\n",
    "# Exclude Light intensity and temperature from targets\n",
    "effects_map = effects_map[[\n",
    "    'control_measurement',\n",
    "    'PSII_closed',\n",
    "    'CBB_inhibited',\n",
    "    'TOX_inhibited',\n",
    "    'electron_drain'\n",
    "]]\n",
    "\n",
    "# Get the effects and map the mto the targets\n",
    "effects = samples.loc[dat.columns.get_level_values(0), [\"Effect in PSET\", \"Treatment\"]]\n",
    "\n",
    "targets = effects_map.loc[pd.MultiIndex.from_frame(effects)].droplevel(1, axis=1)\n",
    "targets = targets.astype(int)\n",
    "targets.index = dat.columns\n",
    "\n",
    "# Make a Multiindex with a duplicated entries\n",
    "target_names = targets.columns\n",
    "targets.columns = pd.MultiIndex.from_arrays([targets.columns, targets.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the conditions as the Multiindex-columns\n",
    "conditions = dat.columns.to_frame()\n",
    "# conditions.index = dat.columns.get_level_values(0)\n",
    "\n",
    "# Select the relevant columns\n",
    "condition_types = pd.Series({\n",
    "    'Strain': \"string\",\n",
    "    'CO2 level': \"numeric\", # There is a meaning to a higher CO2 concentration (maybe make categorical?)\n",
    "    'Cultivation + experiment temperature': \"numeric\",\n",
    "    'Cultivation light intensity': \"numeric\",\n",
    "    'Dark or light acclimated': \"string\",\n",
    "    'Growth light color (nm)': \"string\",\n",
    "    'Fluorometer': \"string\",\n",
    "    'SP color (nm)': \"string\", # There is no linear relationship between wavelength and effect\n",
    "    'SP intensity': \"numeric\",\n",
    "    'OD680 MC-1000': \"numeric\",\n",
    "    'OD720 MC-1000': \"numeric\",\n",
    "})\n",
    "\n",
    "conditions = conditions[condition_types.index]\n",
    "\n",
    "# Replace certain column values\n",
    "\n",
    "# Replace CO2 level with the actual (assumed) numerical ppm\n",
    "conditions[\"CO2 level\"] = conditions[\"CO2 level\"].replace({\n",
    "    \"Air\": \"0.0004\",\n",
    "    \"High CO2\": \"0.05\"\n",
    "}).astype(float)\n",
    "\n",
    "# Replace SP color with categorical value because the numerical gradient is not meaningful\n",
    "conditions[\"SP color (nm)\"] = conditions[\"SP color (nm)\"].astype(str)\n",
    "\n",
    "# Make a Multiindex with a duplicated entries\n",
    "conditions.columns = pd.MultiIndex.from_arrays([conditions.columns, conditions.columns])\n",
    "\n",
    "# Encode conditions in one-hot\n",
    "categorical_conditions = condition_types[condition_types == \"string\"].index.to_numpy()\n",
    "numerical_conditions = condition_types[condition_types != \"string\"].index.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample OJIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of sampled points\n",
    "n_points = 40\n",
    "\n",
    "# Time points, logspaced\n",
    "log_time_points = np.linspace(\n",
    "    np.log10(dat.index[0]),\n",
    "    np.log10(dat.index[-1]),\n",
    "    n_points\n",
    ")\n",
    "time_points = 10 ** log_time_points\n",
    "\n",
    "# Pre-populate the interp function\n",
    "_interp = partial(np.interp, time_points, dat.index)\n",
    "\n",
    "# Interpolate the selected points\n",
    "ojip_sampled = dat.apply(_interp)\n",
    "ojip_sampled.index = pd.MultiIndex.from_product([\n",
    "    [\"ojip\"],\n",
    "    [\"ojip_\" + x for x in log_time_points.round(2).astype(str)]\n",
    "])\n",
    "\n",
    "# Add sampled points to features\n",
    "ojip_sampled = ojip_sampled.T\n",
    "\n",
    "# Subset the data to the samples and time to be included in the analysis \n",
    "ax = dat.plot(legend=False)\n",
    "\n",
    "for t in time_points:\n",
    "    ax.axvline(t)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Add data types\n",
    "ojip_types = pd.Series({\"ojip\":\"time-series-gradients\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all data sets\n",
    "\n",
    "dat_sets = [\n",
    "    conditions,\n",
    "    ojip_sampled,\n",
    "    targets\n",
    "]\n",
    "\n",
    "dat_full = pd.concat(dat_sets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training set\n",
    "dat_train, _dat_trainval = train_test_split(\n",
    "    dat_full,\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=dat_full[target_names]\n",
    ")\n",
    "\n",
    "# Split data into test and validation set\n",
    "dat_test, dat_val = train_test_split(\n",
    "    _dat_trainval,\n",
    "    test_size=0.5, \n",
    "    random_state=42,\n",
    "    stratify=_dat_trainval[target_names]\n",
    ")\n",
    "\n",
    "print(f\"train: {dat_train.shape}, test: {dat_test.shape}, val: {dat_val.shape}\")\n",
    "\n",
    "\n",
    "if not np.all(dat_train[target_names].drop_duplicates().sum(axis=0) == 1):\n",
    "    raise RuntimeError(\"Not all targets are in the training set\")\n",
    "\n",
    "# Make into dataset\n",
    "train_ds = fnc.df_to_dataset(dat_train, targets=target_names)\n",
    "test_ds = fnc.df_to_dataset(dat_test, targets=target_names)\n",
    "val_ds = fnc.df_to_dataset(dat_val, targets=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the types of all features\n",
    "feature_types = pd.concat([\n",
    "    condition_types,\n",
    "    ojip_types\n",
    "])\n",
    "\n",
    "# Create containers for inputs and encodings\n",
    "all_inputs = {}\n",
    "encoded_features_dict = {}\n",
    "encoded_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name, col_dtype in feature_types.items():\n",
    "\n",
    "    print(col_name)\n",
    "    # Create a numeric normalisation layer\n",
    "    if col_dtype == \"numeric\":\n",
    "        col = layers.Input(shape=(1,), name=col_name)\n",
    "        normalization_layer = fnc.get_normalization_layer(col_name, train_ds)\n",
    "        encoded_col = normalization_layer(col)\n",
    "    \n",
    "    # Create a string enconding layer, could also work for integer encoding\n",
    "    elif col_dtype == \"string\":\n",
    "        col = layers.Input(shape=(1,), name=col_name, dtype='string')\n",
    "        encoding_layer = fnc.get_category_encoding_layer(name=col_name,\n",
    "                                                    dataset=train_ds,\n",
    "                                                    dtype='string',\n",
    "                                                    max_tokens=5)\n",
    "        encoded_col = encoding_layer(col)\n",
    "\n",
    "    # Create a layer to normalise time series and calculate gradients\n",
    "    elif col_dtype == \"time-series-gradients\":\n",
    "        col = layers.Input(shape=(dat_full[col_name].shape[1],), name=col_name)\n",
    "        reshaped_col = layers.Reshape((dat_full[col_name].shape[1], 1))(col)\n",
    "        normalization_layer = fnc.NormalizedTimeSeriesWithDerivatives()\n",
    "        encoded_col = normalization_layer(reshaped_col)\n",
    "        encoded_col = layers.Flatten()(encoded_col)\n",
    "        \n",
    "    else:\n",
    "        raise KeyError(f\"No handling for col_dtype {col_dtype} defined\")\n",
    "\n",
    "    all_inputs[col_name] = col\n",
    "    encoded_features.append(encoded_col)\n",
    "    encoded_features_dict[col_name] = encoded_col\n",
    "\n",
    "# Define the preprocessing layer\n",
    "preprocessing_layer = keras.Model(\n",
    "    all_inputs,\n",
    "    encoded_features_dict,\n",
    "    name=\"preprocessing_layer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    preprocessing_layer,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    # rankdir=\"LR\",\n",
    "    to_file=\"figures/test_preprocessing.png\",\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create UMAP mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe into model input dict\n",
    "def get_inputdict_from_df(df, all_inputs):\n",
    "    return {key: df[key].to_numpy() for key in all_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the outputs of the preprocessing layer to perform UMAP\n",
    "concatenated_preprocess = keras.Model(\n",
    "    all_inputs,\n",
    "    layers.concatenate(list(preprocessing_layer(all_inputs).values())),\n",
    "    name=\"concatenation_layer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for UMAP\n",
    "UMAP_seed = 2025\n",
    "\n",
    "# Scale the features\n",
    "df_features_scaled = concatenated_preprocess(get_inputdict_from_df(dat_full, all_inputs)).numpy()\n",
    "\n",
    "# Create the UMAP embedding\n",
    "reducer = umap.UMAP(random_state=UMAP_seed)\n",
    "embedding = pd.DataFrame(\n",
    "    reducer.fit_transform(df_features_scaled),\n",
    "    index=dat_full.index,\n",
    "    columns=[\"UMAP_1\", \"UMAP_2\"]\n",
    ").reset_index()\n",
    "\n",
    "# Plot\n",
    "categories = df.columns.names[1:]\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,15),\n",
    "    sharey=True,\n",
    "    sharex=True,\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot targest on UMAP\n",
    "# Add UMAP to targets\n",
    "embedding_targets = pd.concat([\n",
    "    targets.droplevel(-1, axis=1).droplevel(list(range(1,20)), axis=0),\n",
    "    embedding.set_index(\"Label\").loc[:, [\"UMAP_1\", \"UMAP_2\"]],\n",
    "], axis=1)\n",
    "\n",
    "# Plot\n",
    "categories = effects_map.columns.get_level_values(0)\n",
    "fig, axes = plt.subplots(\n",
    "    int(np.ceil(len(categories)/3)),\n",
    "    3,\n",
    "    figsize=(7,5),\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "for category, ax in zip(categories, axes.flatten()):\n",
    "    sns.scatterplot(\n",
    "        embedding_targets,\n",
    "        x=\"UMAP_1\",\n",
    "        y=\"UMAP_2\",\n",
    "        hue=category,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(category)\n",
    "\n",
    "    if len(embedding_targets[category].value_counts()) == 1:\n",
    "        ax.text(s=\"one category\",x=0.98, y=0.98, ha=\"right\", va=\"top\", transform=ax.transAxes, size=7)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "all_features = preprocessing_layer(all_inputs)\n",
    "all_features = layers.concatenate(list(all_features.values()))\n",
    "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "dense_output = layers.Dense(targets.shape[1], activation=\"sigmoid\")(x)\n",
    "\n",
    "outputs = {\n",
    "    \"control_measurement\": layers.Lambda(lambda x: tf.expand_dims(x[:, 0], axis=-1), name=\"control_measurement\")(dense_output),\n",
    "    \"PSII_closed\": layers.Lambda(lambda x: tf.expand_dims(x[:, 1], axis=-1), name=\"PSII_closed\")(dense_output),\n",
    "    \"CBB_inhibited\": layers.Lambda(lambda x: tf.expand_dims(x[:, 2], axis=-1), name=\"CBB_inhibited\")(dense_output),\n",
    "    \"TOX_inhibited\": layers.Lambda(lambda x: tf.expand_dims(x[:, 3], axis=-1), name=\"TOX_inhibited\")(dense_output),\n",
    "    \"electron_drain\": layers.Lambda(lambda x: tf.expand_dims(x[:, 4], axis=-1), name=\"electron_drain\")(dense_output),\n",
    "}\n",
    "\n",
    "model = keras.Model(all_inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile test model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics={key:[\n",
    "        keras.metrics.BinaryAccuracy(threshold=0.5),\n",
    "        keras.metrics.Recall(thresholds=0.5),\n",
    "        keras.metrics.Precision(thresholds=0.5),\n",
    "        keras.metrics.F1Score(threshold=0.5),\n",
    "        ] for key in target_names},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `rankdir='LR'` to make the graph horizontal.\n",
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    # rankdir=\"LR\",\n",
    "    to_file=\"figures/model.png\",\n",
    "    dpi=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[TqdmCallback(verbose=1)]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "# loss, mae = model.evaluate(X_test_scaled, Y_test_scaled, verbose=0)\n",
    "# print(f\"Mean Absolute Error on Test Set: {mae}\")\n",
    "\n",
    "# Plot the loss over the Epochs\n",
    "fnc.plot_loss_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(test_ds.map(lambda x,y : x))\n",
    "test = pd.DataFrame({k:v.flatten() for k,v in test.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.map(lambda x,y : y).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.predict(test_ds.map(lambda x,y : x)))\n",
    "# Does not look correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc.get_model_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_working = []\n",
    "# outputs = {}\n",
    "# for i in range(len(input_dict)):\n",
    "#     try:\n",
    "#         test_feature = list(input_dict.keys())[i]\n",
    "\n",
    "#         test_model = keras.Model({test_feature:all_inputs[test_feature]}, encoded_features_dict[test_feature])\n",
    "#         test_model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "#         outputs[test_feature] = test_model.predict({test_feature:input_dict[test_feature]})\n",
    "#     except Exception:\n",
    "#         not_working.append(test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model\n",
    "# Define the feature inputs\n",
    "ojip_input = Input(shape=(X_train_scaled.shape[1],), name=\"ojip_input\")\n",
    "\n",
    "x = keras.layers.Reshape((X_train_scaled.shape[1],1), name=\"LSTM_1_reshape\")(ojip_input)\n",
    "\n",
    "# Hidden layer\n",
    "x = LSTM(64, activation=\"tanh\", name=\"LSTM_1\")(x)\n",
    "x = Dropout(0.3, name=\"LSTM_1_Dropout\")(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(Y_train_scaled.shape[1], activation=\"relu\", name=\"prediction\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[ojip_input],\n",
    "    outputs=[output],\n",
    ")\n",
    "\n",
    "\n",
    "##  Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError]\n",
    ")\n",
    "\n",
    "##  Train the model\n",
    "history = model.fit(\n",
    "    [X_train_scaled],\n",
    "    [Y_train_scaled],\n",
    "    validation_split=0.1,\n",
    "    epochs=500,\n",
    "    verbose=0,\n",
    "    batch_size=10,\n",
    "    callbacks=[TqdmCallback(verbose=1)]\n",
    ")\n",
    "\n",
    "plot_loss_development(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(models_metrics), sharex=True)\n",
    "\n",
    "for model, ax in zip(models_metrics, axes.flatten()):\n",
    "    # Plot the model metrics\n",
    "    models_metrics[model].plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.compile(loss='mae', optimizer='adam')\n",
    "# model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=0)\n",
    "\n",
    "# print(model.evaluate(X_test, y_test))\n",
    "# # 10.704551696777344\n",
    "\n",
    "# # normalize the inputs outside the model\n",
    "# normalizer = Normalization()\n",
    "# normalizer.adapt(X_train)\n",
    "\n",
    "# X_train_normalized = normalizer(X_train)\n",
    "# X_test_normalized = normalizer(X_test)\n",
    "\n",
    "# inputs = Input(shape=[None, 1])\n",
    "# x = LSTM(4, return_sequences=True)(inputs)\n",
    "# x = LSTM(2, return_sequences=True)(x)\n",
    "# x = LSTM(2, return_sequences=True)(x)\n",
    "# x = LSTM(4, return_sequences=True)(x)\n",
    "# x = TimeDistributed((Dense(1)))(x)\n",
    "# model = Model(inputs, x)\n",
    "\n",
    "# model.compile(loss='mae', optimizer='adam')\n",
    "# model.fit(X_train_normalized, y_train, batch_size=32, epochs=10, verbose=0)\n",
    "\n",
    "# print(model.evaluate(X_test_normalized, y_test))\n",
    "# # 10.748750686645508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class TimeSeriesNormalization(layers.Layer):\n",
    "#     def __init__(self, epsilon=1e-6):\n",
    "#         super(TimeSeriesNormalization, self).__init__()\n",
    "#         self.epsilon = epsilon  # To prevent division by zero\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         \"\"\"\n",
    "#         Normalize each time series independently to zero mean and unit variance.\n",
    "\n",
    "#         Args:\n",
    "#             inputs: Tensor of shape (batch_size, time_steps, features)\n",
    "\n",
    "#         Returns:\n",
    "#             Normalized tensor of the same shape\n",
    "#         \"\"\"\n",
    "#         mean = tf.reduce_mean(inputs, axis=1, keepdims=True)  # Compute mean along time axis\n",
    "#         std = tf.math.reduce_std(inputs, axis=1, keepdims=True)  # Compute std along time axis\n",
    "\n",
    "#         return (inputs - mean) / (std + self.epsilon)  # Normalize\n",
    "\n",
    "# # Example usage\n",
    "# batch_size, time_steps, features = 32, 100, 5\n",
    "# input_data = tf.random.normal((batch_size, time_steps, features))  # Simulated time series data\n",
    "\n",
    "# normalization_layer = TimeSeriesNormalization()\n",
    "# normalized_data = normalization_layer(input_data)\n",
    "\n",
    "# print(\"Input shape:\", input_data.shape)\n",
    "# print(\"Normalized shape:\", normalized_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots()\n",
    "# ax.plot(normalized_data.numpy().std(axis=1))\n",
    "# ax.set_ylim(-1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojipml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
